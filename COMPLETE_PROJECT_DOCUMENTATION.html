<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Checkers ML D3QN Complete Project Documentation</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
        }
        
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 12px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }
        
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 60px 40px;
            text-align: center;
        }
        
        header h1 {
            font-size: 3em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }
        
        header p {
            font-size: 1.2em;
            opacity: 0.95;
            margin-bottom: 20px;
        }
        
        .meta-info {
            display: flex;
            justify-content: center;
            gap: 30px;
            margin-top: 20px;
            flex-wrap: wrap;
        }
        
        .meta-item {
            background: rgba(255,255,255,0.1);
            padding: 10px 20px;
            border-radius: 6px;
            font-size: 0.95em;
        }
        
        nav {
            background: #f8f9fa;
            padding: 20px 40px;
            border-bottom: 3px solid #667eea;
            position: sticky;
            top: 0;
            z-index: 100;
        }
        
        nav ul {
            list-style: none;
            display: flex;
            gap: 30px;
            flex-wrap: wrap;
            max-width: 1200px;
            margin: 0 auto;
        }
        
        nav a {
            text-decoration: none;
            color: #667eea;
            font-weight: 600;
            transition: color 0.3s;
        }
        
        nav a:hover {
            color: #764ba2;
        }
        
        .content {
            padding: 40px;
            max-width: 1200px;
            margin: 0 auto;
        }
        
        section {
            margin-bottom: 60px;
            scroll-margin-top: 100px;
        }
        
        h2 {
            font-size: 2.2em;
            color: #667eea;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #667eea;
        }
        
        h3 {
            font-size: 1.5em;
            color: #764ba2;
            margin-top: 30px;
            margin-bottom: 15px;
        }
        
        h4 {
            font-size: 1.2em;
            color: #333;
            margin-top: 20px;
            margin-bottom: 10px;
        }
        
        p {
            margin-bottom: 15px;
            line-height: 1.8;
        }
        
        .highlight {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }
        
        .success {
            background: #d4edda;
            border-left: 4px solid #28a745;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
            color: #155724;
        }
        
        .warning {
            background: #f8d7da;
            border-left: 4px solid #dc3545;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
            color: #721c24;
        }
        
        .info {
            background: #d1ecf1;
            border-left: 4px solid #17a2b8;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
            color: #0c5460;
        }
        
        code {
            background: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            color: #d63384;
        }
        
        pre {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 6px;
            overflow-x: auto;
            margin: 20px 0;
            line-height: 1.4;
        }
        
        pre code {
            background: none;
            color: inherit;
            padding: 0;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        table th {
            background: #667eea;
            color: white;
            padding: 12px;
            text-align: left;
            font-weight: 600;
        }
        
        table td {
            padding: 12px;
            border-bottom: 1px solid #ddd;
        }
        
        table tr:hover {
            background: #f5f5f5;
        }
        
        .grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        
        .card {
            background: #f8f9fa;
            border: 2px solid #667eea;
            border-radius: 8px;
            padding: 20px;
            transition: transform 0.3s, box-shadow 0.3s;
        }
        
        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 20px rgba(102, 126, 234, 0.2);
        }
        
        .card h4 {
            color: #667eea;
            margin-top: 0;
        }
        
        .timeline {
            position: relative;
            padding: 20px 0;
        }
        
        .timeline-item {
            padding-left: 40px;
            margin-bottom: 30px;
            position: relative;
        }
        
        .timeline-item::before {
            content: '';
            position: absolute;
            left: 0;
            top: 5px;
            width: 20px;
            height: 20px;
            background: #667eea;
            border-radius: 50%;
            border: 3px solid white;
            box-shadow: 0 0 0 3px #667eea;
        }
        
        .timeline-item::after {
            content: '';
            position: absolute;
            left: 8px;
            top: 30px;
            width: 4px;
            height: 100%;
            background: #667eea;
            opacity: 0.3;
        }
        
        .timeline-item:last-child::after {
            display: none;
        }
        
        .timeline-label {
            font-weight: 700;
            color: #667eea;
            font-size: 0.95em;
            margin-bottom: 5px;
        }
        
        .architecture-box {
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            border: 2px solid #667eea;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
            font-family: monospace;
            white-space: pre-wrap;
            overflow-x: auto;
        }
        
        .stat-block {
            display: inline-block;
            background: white;
            border: 2px solid #667eea;
            border-radius: 8px;
            padding: 20px 30px;
            margin: 10px 10px 10px 0;
            text-align: center;
        }
        
        .stat-number {
            font-size: 2em;
            font-weight: 700;
            color: #667eea;
        }
        
        .stat-label {
            font-size: 0.9em;
            color: #666;
            margin-top: 5px;
        }
        
        ul, ol {
            margin-left: 20px;
            margin-bottom: 15px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        .generation-block {
            background: white;
            border-left: 5px solid #667eea;
            padding: 20px;
            margin: 20px 0;
            border-radius: 4px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        .generation-block.gen1 { border-left-color: #ff6b6b; }
        .generation-block.gen2 { border-left-color: #4ecdc4; }
        .generation-block.gen3 { border-left-color: #45b7d1; }
        .generation-block.gen4 { border-left-color: #96ceb4; }
        .generation-block.gen5 { border-left-color: #ffeaa7; }
        .generation-block.gen6 { border-left-color: #dda0dd; }
        .generation-block.gen7 { border-left-color: #98d8c8; }
        .generation-block.gen10 { border-left-color: #f7b731; }
        .generation-block.gen11 { border-left-color: #5f27cd; }
        
        .generation-title {
            font-size: 1.3em;
            font-weight: 700;
            margin-bottom: 15px;
            color: #333;
        }
        
        footer {
            background: #2c3e50;
            color: white;
            text-align: center;
            padding: 30px 40px;
            margin-top: 60px;
        }
        
        footer a {
            color: #667eea;
            text-decoration: none;
        }
        
        footer a:hover {
            text-decoration: underline;
        }
        
        @media print {
            body { background: white; }
            nav { position: static; }
            .container { box-shadow: none; }
        }
        
        @media (max-width: 768px) {
            header { padding: 40px 20px; }
            header h1 { font-size: 2em; }
            nav ul { gap: 15px; }
            .content { padding: 20px; }
            h2 { font-size: 1.8em; }
            h3 { font-size: 1.3em; }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>üéÆ Checkers ML: D3QN Complete Journey</h1>
            <p>Comprehensive Documentation of AI Agent Development & Training Evolution</p>
            <div class="meta-info">
                <div class="meta-item">üìä Project Type: Deep Reinforcement Learning</div>
                <div class="meta-item">üéØ Objective: Self-Play Checkers AI</div>
                <div class="meta-item">‚öôÔ∏è Framework: PyTorch + D3QN</div>
                <div class="meta-item">üíª Hardware: RTX 2060 + I7 10th Gen</div>
            </div>
        </header>
        
        <nav>
            <ul>
                <li><a href="#overview">Overview</a></li>
                <li><a href="#architecture">Architecture</a></li>
                <li><a href="#phases">Training Phases</a></li>
                <li><a href="#generations">Generations</a></li>
                <li><a href="#discoveries">Key Discoveries</a></li>
                <li><a href="#infrastructure">Infrastructure</a></li>
                <li><a href="#evaluation">Evaluation</a></li>
                <li><a href="#conclusion">Conclusion</a></li>
            </ul>
        </nav>
        
        <div class="content">
            
            <!-- OVERVIEW SECTION -->
            <section id="overview">
                <h2>üìã Project Overview</h2>
                
                <p>This documentation covers the complete journey of developing a Checkers-playing AI agent using <strong>Dueling Double Deep Q-Network (D3QN)</strong> reinforcement learning. The project demonstrates iterative development, debugging, and optimization of a production-grade RL system.</p>
                
                <h3>Project Goals</h3>
                <ul>
                    <li>Build a self-learning checkers AI agent using deep reinforcement learning</li>
                    <li>Implement Double DQN with Dueling architecture for improved Q-value estimates</li>
                    <li>Create a robust training infrastructure with experience replay and legal move masking</li>
                    <li>Identify and overcome reward hacking and training instabilities</li>
                    <li>Achieve competitive performance against random baselines (Note: Goal is technical competency, not superhuman play)</li>
                    <li>Enable self-play training with curriculum learning</li>
                </ul>
                
                <h3>Key Achievements & Lessons</h3>
                <div class="grid">
                    <div class="card">
                        <h4>‚úÖ Stable Infrastructure</h4>
                        <p>Built a production-ready D3QN pipeline that solved initial reward hacking and numerical instability.</p>
                    </div>
                    <div class="card">
                        <h4>üèÜ High Baseline WR</h4>
                        <p>Achieved 95%+ win rate against Random Agents (though plateaued against stronger opponents).</p>
                    </div>
                    <div class="card">
                        <h4>üîç Critical Analysis</h4>
                        <p>Identified why self-play fails without a strong teacher (The "Random-Killer" Trap).</p>
                    </div>
                    <div class="card">
                        <h4>‚ö° Reward Engineering</h4>
                        <p>Discovered the "Zero-Sum Draw" flaw that causes passive play in RL agents.</p>
                    </div>
                </div>
                
                <h3>Technical Stack</h3>
                <table>
                    <tr>
                        <th>Component</th>
                        <th>Technology</th>
                        <th>Details</th>
                    </tr>
                    <tr>
                        <td>Framework</td>
                        <td>PyTorch</td>
                        <td>Deep learning, automatic differentiation</td>
                    </tr>
                    <tr>
                        <td>Environment</td>
                        <td>Custom Checkers Engine</td>
                        <td>8√ó8 board, mandatory captures, king rules</td>
                    </tr>
                    <tr>
                        <td>Agent Architecture</td>
                        <td>Dueling Double DQN</td>
                        <td>Value + Advantage streams, Double Q-learning</td>
                    </tr>
                    <tr>
                        <td>GPU Acceleration</td>
                        <td>CUDA RTX 2060</td>
                        <td>6GB VRAM, optimized kernel utilization</td>
                    </tr>
                    <tr>
                        <td>Memory Management</td>
                        <td>CPU Replay Buffer</td>
                        <td>100K transitions in system RAM, batch transfer to GPU</td>
                    </tr>
                </table>
            </section>
            
            <!-- ARCHITECTURE SECTION -->
            <section id="architecture">
                <h2>üèóÔ∏è Neural Network Architecture</h2>
                
                <h3>Model Overview</h3>
                <p>The D3QN model combines three key innovations in deep Q-learning:</p>
                <ol>
                    <li><strong>Dueling Architecture:</strong> Separate value and advantage streams for better credit assignment</li>
                    <li><strong>Double DQN:</strong> Decoupled action selection and evaluation to reduce Q-value overestimation</li>
                    <li><strong>Convolutional Layers:</strong> Exploit spatial structure of checkers board</li>
                </ol>
                
                <h3>Network Architecture Details</h3>
                
                <div class="architecture-box">Input Layer: (Batch, 5, 8, 8)
    ‚îú‚îÄ Channel 0: Own regular pieces
    ‚îú‚îÄ Channel 1: Own kings
    ‚îú‚îÄ Channel 2: Opponent regular pieces
    ‚îú‚îÄ Channel 3: Opponent kings
    ‚îî‚îÄ Channel 4: Tempo indicator (whose turn)
    
Feature Extraction: Convolutional Stack
    ‚îú‚îÄ Conv2d(5‚Üí32, kernel=3√ó3, padding=1)
    ‚îÇ   ‚îî‚îÄ Output: (Batch, 32, 8, 8)
    ‚îÇ   ‚îî‚îÄ Activation: ReLU
    ‚îÇ
    ‚îú‚îÄ Conv2d(32‚Üí64, kernel=3√ó3, padding=1)
    ‚îÇ   ‚îî‚îÄ Output: (Batch, 64, 8, 8)
    ‚îÇ   ‚îî‚îÄ Activation: ReLU
    ‚îÇ
    ‚îî‚îÄ Conv2d(64‚Üí64, kernel=3√ó3, padding=1)
        ‚îî‚îÄ Output: (Batch, 64, 8, 8)
        ‚îî‚îÄ Activation: ReLU

Flattening & Normalization
    ‚îú‚îÄ Flatten: 64 √ó 8 √ó 8 = 4096 features
    ‚îú‚îÄ LayerNorm(4096): Normalize mean=0, std=1
    ‚îî‚îÄ Shared FC(4096‚Üí256): Common feature compression

Dueling Streams (Parallel Processing)
    ‚îú‚îÄ Value Stream (State Evaluation)
    ‚îÇ   ‚îú‚îÄ FC(256‚Üí128)
    ‚îÇ   ‚îú‚îÄ ReLU
    ‚îÇ   ‚îî‚îÄ FC(128‚Üí1) ‚Üí V(s)
    ‚îÇ
    ‚îî‚îÄ Advantage Stream (Action Advantage)
        ‚îú‚îÄ FC(256‚Üí128)
        ‚îú‚îÄ ReLU
        ‚îî‚îÄ FC(128‚Üí168) ‚Üí A(s,a)

Q-Value Aggregation
    Q(s,a) = V(s) + [A(s,a) - mean(A(s,a))]
    ‚îî‚îÄ Output scaling: √ó 0.1 (prevent explosion)

Final Output: (Batch, 168) - Q-values for all actions</div>
                
                <h3>Architecture Parameters</h3>
                <div class="grid">
                    <div class="card">
                        <h4>Model Size</h4>
                        <div class="stat-number">~3.5M</div>
                        <div class="stat-label">Trainable Parameters</div>
                    </div>
                    <div class="card">
                        <h4>Action Space</h4>
                        <div class="stat-number">168</div>
                        <div class="stat-label">Discrete Actions</div>
                    </div>
                    <div class="card">
                        <h4>Conv Layers</h4>
                        <div class="stat-number">3</div>
                        <div class="stat-label">Convolutional Layers</div>
                    </div>
                    <div class="card">
                        <h4>Hidden Units</h4>
                        <div class="stat-number">128</div>
                        <div class="stat-label">Dense Layer Size</div>
                    </div>
                </div>
                
                <h3>Key Design Decisions</h3>
                <table>
                    <tr>
                        <th>Component</th>
                        <th>Design Choice</th>
                        <th>Rationale</th>
                    </tr>
                    <tr>
                        <td>Input Encoding</td>
                        <td>5-channel board representation</td>
                        <td>Separate channels for piece types + tempo information</td>
                    </tr>
                    <tr>
                        <td>Convolutional Layers</td>
                        <td>3 Conv layers (5‚Üí32‚Üí64‚Üí64)</td>
                        <td>Balance feature extraction with parameter efficiency</td>
                    </tr>
                    <tr>
                        <td>Dueling Streams</td>
                        <td>128-unit hidden layer each</td>
                        <td>Sufficient capacity for value and advantage estimation</td>
                    </tr>
                    <tr>
                        <td>Output Scaling</td>
                        <td>0.1√ó multiplier on Q-values</td>
                        <td>Prevent Q-value explosion from reward accumulation</td>
                    </tr>
                    <tr>
                        <td>Normalization</td>
                        <td>LayerNorm after flatten</td>
                        <td>Stabilize feature magnitudes before dense layers</td>
                    </tr>
                </table>
                
                <h3>Initialization Strategy</h3>
                <p>Network weights are initialized using <strong>Kaiming initialization</strong> (He initialization) to maintain stable gradient flow during training:</p>
                <ul>
                    <li><strong>Convolutional layers:</strong> <code>nn.init.kaiming_normal_(weight, nonlinearity='relu')</code></li>
                    <li><strong>Dense layers:</strong> <code>nn.init.kaiming_normal_(weight, nonlinearity='relu')</code></li>
                    <li><strong>Biases:</strong> Initialized to zero</li>
                </ul>
                
                <h3>Online and Target Networks</h3>
                <p>The D3QN algorithm maintains two separate network copies:</p>
                <ul>
                    <li><strong>Online Network:</strong> Updated every training step via gradient descent</li>
                    <li><strong>Target Network:</strong> Updated via Polyak averaging (soft update) with œÑ=0.005</li>
                </ul>
                <p>This separation prevents catastrophic instability from moving targets during Q-learning updates.</p>
            </section>
            
            <!-- TRAINING PHASES SECTION -->
            <section id="phases">
                <h2>üìà Training Phases & Evolution</h2>
                
                <h3>Phase Architecture Overview</h3>
                <p>The project progressed through distinct phases, each building on lessons from the previous:</p>
                
                <div class="timeline">
                    <div class="timeline-item">
                        <div class="timeline-label">Phase 1-2: Interface & Network Development</div>
                        <p>Developed core components (ActionManager, BoardEncoder, MoveParser) and implemented DuelingDQN architecture with proper initialization.</p>
                        <p><strong>Outcome:</strong> Foundation for stable training pipeline</p>
                    </div>
                    <div class="timeline-item">
                        <div class="timeline-label">Phase 3: Training Infrastructure</div>
                        <p>Built ReplayBuffer with pre-allocated arrays and D3QNTrainer with complete training loop, Double DQN, legal action masking.</p>
                        <p><strong>Outcome:</strong> Production-ready training system for 100K+ episodes</p>
                    </div>
                    <div class="timeline-item">
                        <div class="timeline-label">Gen 3 Training: Empirical Discovery</div>
                        <p>Initial training runs revealed reward hacking phenomenon where agent optimized for captures rather than wins.</p>
                        <p><strong>Outcome:</strong> Identified root cause of poor performance (misaligned reward function)</p>
                    </div>
                    <div class="timeline-item">
                        <div class="timeline-label">Gen 6-7: Reward Fixes</div>
                        <p>Iteratively improved reward function with Gen 6.5 "Logical Economy" system based on empirical reward distributions.</p>
                        <p><strong>Outcome:</strong> Stabilized training with 90%+ win rates</p>
                    </div>
                    <div class="timeline-item">
                        <div class="timeline-label">Gen 10+: Self-Play & Curriculum</div>
                        <p>Implemented self-play training with opponent pool, curriculum learning, and Iron League tournament system.</p>
                        <p><strong>Outcome:</strong> Multi-agent competitive training framework</p>
                    </div>
                </div>
                
                <h3>Hyperparameter Evolution</h3>
                <table>
                    <tr>
                        <th>Hyperparameter</th>
                        <th>Phase 1-3</th>
                        <th>Gen 6-7</th>
                        <th>Gen 10-11</th>
                        <th>Rationale for Changes</th>
                    </tr>
                    <tr>
                        <td>Learning Rate</td>
                        <td>2√ó10‚Åª‚Åµ</td>
                        <td>1√ó10‚Åª‚Å¥</td>
                        <td>5√ó10‚Åª‚Åµ</td>
                        <td>Increased for faster convergence, then balanced for stability</td>
                    </tr>
                    <tr>
                        <td>Batch Size</td>
                        <td>64</td>
                        <td>128</td>
                        <td>128</td>
                        <td>Increased for better gradient estimates</td>
                    </tr>
                    <tr>
                        <td>Œ≥ (Discount)</td>
                        <td>0.99</td>
                        <td>0.99</td>
                        <td>0.995</td>
                        <td>Increased to value long-term returns in self-play</td>
                    </tr>
                    <tr>
                        <td>Œµ Initial</td>
                        <td>1.0</td>
                        <td>1.0</td>
                        <td>1.0</td>
                        <td>Consistent exploration-exploitation tradeoff</td>
                    </tr>
                    <tr>
                        <td>Œµ Final</td>
                        <td>0.05</td>
                        <td>0.05</td>
                        <td>0.01</td>
                        <td>Lower final Œµ for more exploitation in later stages</td>
                    </tr>
                    <tr>
                        <td>Soft Update œÑ</td>
                        <td>0.005</td>
                        <td>0.005</td>
                        <td>0.001</td>
                        <td>More frequent target updates for self-play stability</td>
                    </tr>
                </table>
            </section>
            
            <!-- GENERATIONS SECTION -->
            <section id="generations">
                <h2>üéØ Training Generations & Results</h2>
                
                <h3>Generation Overview</h3>
                <p>The project evolved through 11 distinct generations, each representing significant changes to architecture, rewards, or training approach:</p>
                
                <div class="generation-block gen1">
                    <div class="generation-title">Generation 0-1: Baseline Implementation</div>
                    <p><strong>Objective:</strong> Establish working D3QN training loop</p>
                    <p><strong>Architecture:</strong> Basic Conv‚ÜíFlatten‚ÜíDueling structure</p>
                    <p><strong>Reward System:</strong> Simple: Win=+100, Loss=-75, Captures=+5</p>
                    <p><strong>Results:</strong> Initial convergence to ~60% win rate, then degradation</p>
                    <p><strong>Episodes Trained:</strong> ~2,000</p>
                    <p><strong>Key Lesson:</strong> Basic reward structure insufficient; agent learns shortcuts</p>
                </div>
                
                <div class="generation-block gen2">
                    <div class="generation-title">Generation 2-3: Reward Shaping Experiments</div>
                    <p><strong>Objective:</strong> Reduce reward hacking through reward shaping</p>
                    <p><strong>Changes:</strong> Added material value tracking, positional rewards, safety penalties</p>
                    <p><strong>Reward Structure:</strong></p>
                    <ul>
                        <li>Terminal: Win +100, Loss -75</li>
                        <li>Per-capture bonus: +5 to +20 (scaled by capture chain length)</li>
                        <li>Positional: +0.5 center control, -0.2 back rank</li>
                        <li>Safety: -0.1 for hanging pieces</li>
                        <li>Living tax: -1 per move</li>
                    </ul>
                    <p><strong>Results:</strong> Peak ~68% win rate at episode 1,500, then collapse to 25%</p>
                    <p><strong>Episodes Trained:</strong> ~5,000</p>
                    <p><strong>Key Lesson:</strong> Discovered reward hacking evidence: positive rewards on losses</p>
                </div>
                
                <div class="generation-block gen3">
                    <div class="generation-title">Generation 3: Explosion & Recovery</div>
                    <p><strong>Objective:</strong> Fix training instability</p>
                    <p><strong>Issues Encountered:</strong></p>
                    <ul>
                        <li>Q-value explosion: values reaching 1000+</li>
                        <li>Loss divergence: NaN and Inf values</li>
                        <li>Agent paralysis: selecting only "safe" zero-reward actions</li>
                    </ul>
                    <p><strong>Fixes Applied:</strong></p>
                    <ul>
                        <li>Gradient clipping: max_norm=0.1 (reduced from 1.0)</li>
                        <li>Output scaling: 0.1√ó multiplier on Q-values</li>
                        <li>Reward scaling: 1/100 applied to all rewards</li>
                        <li>Huber loss instead of MSE for robustness</li>
                    </ul>
                    <p><strong>Results:</strong> Stable loss curves, ~55% win rate after 3,000 episodes</p>
                    <p><strong>Episodes Trained:</strong> ~8,000</p>
                    <p><strong>Key Discovery:</strong> Output scaling critical for numerical stability</p>
                </div>
                
                <div class="generation-block gen4">
                    <div class="generation-title">Generation 4-5: Reward Function Debugging</div>
                    <p><strong>Objective:</strong> Empirically analyze reward distribution and fix pathological behavior</p>
                    <p><strong>Analysis Tools:</strong> Ran 100 random games, logged all rewards received</p>
                    <p><strong>Key Findings:</strong></p>
                    <ul>
                        <li>Reward range: -100 to +200 (winner gets all capture bonuses)</li>
                        <li>Most rewards distributed: -75 (loss), 0 (neutral), +50-100 (wins)</li>
                        <li>Capture chains created unintended value incentives</li>
                    </ul>
                    <p><strong>Fixes:</strong> Introduced empirical thresholds to prevent reward hacking</p>
                    <p><strong>Episodes Trained:</strong> ~6,000</p>
                </div>
                
                <div class="generation-block gen6">
                    <div class="generation-title">Generation 6.5: "Logical Economy" System (Failed)</div>
                    <p><strong>Objective:</strong> Implement principled reward function to prevent exploitation.</p>
                    <p><strong>Architecture:</strong> Four reward tiers based on capture counts (Tier 1-4).</p>
                    <p><strong>Reward Structure:</strong> Win +100, Loss -75, Multi-Jump +20, Single +5.</p>
                    <p><strong>Results:</strong> <span style="color: red; font-weight: bold;">FAILED.</span> Reward hacking persisted.</p>
                    <p><strong>Analysis:</strong> The agent learned to "farm" captures (collecting +20 bonuses) and then intentionally lose (-75), netting positive rewards (+5 to +30) per episode.</p>
                    <p><strong>Episodes Trained:</strong> ~7,000</p>
                    <p><strong>Key Lesson:</strong> "Logical Economy" created a <em>locally optimal adversarial equilibrium</em>.</p>
                </div>
                
                <div class="generation-block gen7">
                    <div class="generation-title">Generation 7: The Restructured Fix</div>
                    <p><strong>Objective:</strong> Definitively solve reward hacking via reward normalization.</p>
                    <p><strong>Changes:</strong></p>
                    <ul>
                        <li><strong>Normalized Rewards:</strong> WIN = +1.0, LOSS = -1.0 (Hard penalty)</li>
                        <li><strong>Auxiliary Captures:</strong> Scaled down to +0.01 (100x smaller)</li>
                        <li><strong>Refined Epsilon:</strong> 1.0 ‚Üí 0.05 over 2,000 episodes</li>
                    </ul>
                    <p><strong>Results:</strong> <span style="color: green; font-weight: bold;">SUCCESS.</span> Consistent 90%+ win rate. Reward hacking impossible due to Loss penalty (-1.0) outweighing any captures.</p>
                    <p><strong>Episodes Trained:</strong> 20,000+</p>
                    <p><strong>Key Metric:</strong> <code>Best Model @ Ep. 18,500: 96.2% win rate vs random</code></p>
                </div>
                
                <div class="generation-block gen8">
                    <div class="generation-title">Generation 8: "Titan" Baseline</div>
                    <p><strong>Identity:</strong> <code>gen8_titan</code> (Checkpoint @ Ep 2,500)</p>
                    <p><strong>Role:</strong> Previous main champion, now retired to the Opponent Pool as a strong baseline.</p>
                    <p><strong>Performance:</strong></p>
                    <ul>
                        <li><strong>Tournament Rank:</strong> 4th (58.3% Win Rate)</li>
                        <li><strong>Vs Random:</strong> ~90% Win Rate</li>
                    </ul>
                    <p><strong>Architecture:</strong> Dueling D3QN with ~168 discrete action IDs.</p>
                    <p><strong>Usage:</strong> Serves as the primary evaluation baseline for all subsequent generations.</p>
                </div>

                <div class="generation-block gen9">
                    <div class="generation-title">Generation 9: "Titan" Iron League Champion</div>
                    <p><strong>Identity:</strong> <code>gen9_titan</code> (Checkpoint @ Ep 4,500)</p>
                    <p><strong>Status:</strong> <span style="color: gray; font-weight: bold;">FORMER CHAMPION (Surpassed).</span> Once the best deployment model, now consistently beaten by the simpler Gen 7 baseline.</p>
                    <p><strong>Tournament Dominance:</strong></p>
                    <ul>
                        <li><strong>Rank:</strong> 3rd of 13 Agents</li>
                        <li><strong>Win Rate:</strong> 45.8% (Fair Tournament)</li>
                        <li><strong>Head-to-Head:</strong> 7.5 - 2.5 vs Gen 8 Titan, but loses to Gen 7.</li>
                    </ul>
                    <p><strong>Training Config (Phase 5):</strong></p>
                    <ul>
                        <li><strong>Q-Health Monitor:</strong> Auto-brake engaged when Max Q > 30.</li>
                        <li><strong>Emergency Stop:</strong> Halted at Ep 4,700 (Max Q > 50) to prevent divergence.</li>
                        <li><strong>Opponents:</strong> Mixed pool of Gen 7, 8, and early Gen 9.</li>
                    </ul>
                    <p><strong>Key Metric:</strong> <code>85.0% Win Rate vs Random (100 games)</code></p>
                </div>
                
                <div class="generation-block gen10">
                    <div class="generation-title">Generation 10: "Titan" Endgame Curriculum (Failed)</div>
                    <p><strong>Objective:</strong> Fix late-game collapse via "Endgame Injection" curriculum.</p>
                    <p><strong>Architecture Changes:</strong></p>
                    <ul>
                        <li>Opponent pool system with 60% pool / 30% self-play mix.</li>
                        <li><strong>Endgame Injection:</strong> 40% of games started with 6-10 pieces (synthetic endgames).</li>
                        <li><strong>Auto-Brake:</strong> Learning rate reduction trigger to stop Q-value explosion.</li>
                    </ul>
                    <p><strong>Results:</strong> <span style="color: red; font-weight: bold;">FAILED.</span> Performance degraded vs Gen 9.</p>
                    <p><strong>Failure Analysis:</strong></p>
                    <ul>
                        <li><strong>Late-Game Collapse:</strong> Agent couldn't correlate early king positioning with endgame outcomes due to temporal credit assignment failure.</li>
                        <li><strong>Learning Rate Crash:</strong> "Auto-brake" system was too aggressive (threshold 35.0), collapsing LR to 1e-8 and stopping learning.</li>
                        <li><strong>Outcome:</strong> Gen 11 and Gen 9 remained dominant. Gen 10 became an "Endgame Specialist" that lost the opening.</li>
                    </ul>
                    <p><strong>Episodes Trained:</strong> 50,000+</p>
                </div>
                
                <div class="generation-block gen11">
                    <div class="generation-title">Generation 11: The Decisive Update (Amplified Rewards)</div>
                    <p><strong>Objective:</strong> Solve the "Draw Stagnation" and "Timeout Epidemic" via aggressive reward shaping.</p>
                    <p><strong>Problem Identified:</strong> Previous agents learned to stall (16.3% timeout rate) because the living penalty was too small (-0.0001) relative to the risk of losing.</p>
                    <p><strong>Critical Fixes:</strong></p>
                    <ul>
                        <li><strong>Amplified Captures:</strong> Single (+0.25) and Multi (+0.50) rewards increased 3x to dominate Q-values.</li>
                        <li><strong>Harsh Living Tax:</strong> Increased to -0.01 per move (100x stronger), forcing rapid play.</li>
                        <li><strong>Draw Penalty:</strong> Equalized to -1.0 (same as Loss), making stalling unviable.</li>
                        <li><strong>Dynamic Pool:</strong> Rolling window updates every 500 episodes to prevent stale meta.</li>
                    </ul>
                    <p><strong>Results:</strong> Timeouts reduced to 0.8%. Game length stabilized at 65 steps.</p>
                    <p><strong>Status:</strong> Decisive D3QN. Successfully solved the passive play issue, but strategic depth remains limited compared to human analysis.</p>
                </div>

                <div class="generation-block gen12" style="border-left-color: #e74c3c;">
                    <div class="generation-title">Generation 12: "Elite" Series (Critical Failure)</div>
                    <p><strong>Objective:</strong> Refine the "Decisive" logic with an elite opponent pool to push for superhuman performance.</p>
                    <p><strong>Outcome:</strong> <span style="color: red; font-weight: bold;">CATASTROPHIC REGRESSION.</span></p>
                    <p><strong>Tournament Performance:</strong></p>
                    <ul>
                        <li><strong>Win Rates:</strong> 16% - 33% (Bottom of the leaderboard).</li>
                        <li><strong>Analysis:</strong> The models likely overfit to the specific quirks of Gen 11, losing the fundamental solidity of Gen 7. The aggressive "living tax" may have induced reckless behavior when not winning immediately.</li>
                    </ul>
                    <p><strong>Lesson:</strong> Complexity does not equal capability. The simpler Gen 7 model remains superior.</p>
                </div>
                
                <h3>Performance Trajectory</h3>
                <table>
                    <tr>
                        <th>Generation</th>
                        <th>Episodes</th>
                        <th>Peak Win Rate</th>
                        <th>Final Win Rate</th>
                        <th>Training Status</th>
                    </tr>
                    <tr>
                        <td>Gen 1</td>
                        <td>2,000</td>
                        <td>62%</td>
                        <td>45%</td>
                        <td>‚ùå Unstable</td>
                    </tr>
                    <tr>
                        <td>Gen 3</td>
                        <td>5,000</td>
                        <td>68%</td>
                        <td>25%</td>
                        <td>‚ùå Reward Hack</td>
                    </tr>
                    <tr>
                        <td>Gen 6.5</td>
                        <td>7,000</td>
                        <td>85%</td>
                        <td>82%</td>
                        <td>‚ùå Logical Economy Failure</td>
                    </tr>
                    <tr>
                        <td>Gen 7</td>
                        <td>20,000</td>
                        <td>96%</td>
                        <td>94%</td>
                        <td>‚úÖ <strong>Dominant Baseline</strong></td>
                    </tr>
                    <tr>
                        <td>Gen 10</td>
                        <td>50,000</td>
                        <td>93%</td>
                        <td>91%</td>
                        <td>‚ö†Ô∏è High Draw Rate</td>
                    </tr>
                    <tr>
                        <td>Gen 11</td>
                        <td>100,000</td>
                        <td>98%</td>
                        <td>97%</td>
                        <td>‚ö†Ô∏è Good vs Random, Weak vs Gen 7</td>
                    </tr>
                    <tr>
                        <td>Gen 12</td>
                        <td>3,500</td>
                        <td>33%</td>
                        <td>16%</td>
                        <td>üõë <strong>Regression</strong></td>
                    </tr>
                </table>
            </section>
            
            <!-- DISCOVERIES SECTION -->
            <section id="discoveries">
                <h2>üî¨ Key Discoveries & Insights</h2>
                
                <h3>1. The Reward Hacking Phenomenon</h3>
                <div class="highlight">
                    <strong>Discovery:</strong> Agent learned to exploit unintended reward structure by capturing pieces then losing games for positive net rewards.
                </div>
                
                <p><strong>Mathematical Proof:</strong></p>
                <pre>Scenario A (Intended): Win game ‚Üí +100 reward ‚úì

Scenario B (Exploit): 
    ‚Ä¢ Capture 4 pieces (multi-jumps): +80
    ‚Ä¢ Lose game: -75
    ‚Ä¢ Net reward: +5 ‚úì
    
Result: Agent prefers exploited scenario (+5 > 0 if played more efficiently)</pre>
                
                <p><strong>Root Cause:</strong> Reward function incentivized tactical gains independently of strategic outcomes. Capture bonuses (+5 to +20) were too large relative to loss penalty (-75).</p>
                
                <p><strong>Solution:</strong> Implemented "Logical Economy" tier system where bonuses only apply based on game outcome context, preventing exploitation while maintaining learning incentives.</p>
                
                <h3>2. Training Instability & Output Scaling</h3>
                <div class="warning">
                    <strong>Problem:</strong> Q-value explosion (reaching 1000+), leading to NaN losses and training divergence around episode 3,000.
                </div>
                
                <p><strong>Diagnosis:</strong></p>
                <ul>
                    <li>Accumulated reward: Terminal reward (+100) √ó discount chain</li>
                    <li>Q-network output unbounded: No scaling on raw Q-values</li>
                    <li>Gradient explosion: Large errors ‚Üí large gradients ‚Üí overshooting</li>
                </ul>
                
                <p><strong>Fixes Applied (in order of impact):</strong></p>
                <ol>
                    <li><strong>Output Scaling (0.1√ó):</strong> Final layer multiplies Q-values by 0.1, bounding them to [-50, +50]</li>
                    <li><strong>Reward Scaling (1/100):</strong> All rewards divided by 100 before TD error computation</li>
                    <li><strong>Gradient Clipping:</strong> max_norm=0.1 on loss.backward() to prevent overshooting</li>
                    <li><strong>Huber Loss:</strong> More robust to outliers than MSE</li>
                </ol>
                
                <p><strong>Result:</strong> Loss curves stabilized with smooth convergence from 0.8 ‚Üí 0.1 over 5,000 episodes</p>
                
                <h3>3. Legal Action Masking Importance</h3>
                <div class="info">
                    <strong>Insight:</strong> Masking illegal actions during both training and inference is critical for value estimation accuracy.
                </div>
                
                <p><strong>Impact:</strong></p>
                <ul>
                    <li><strong>Without Masking:</strong> Network can select illegal moves during target computation, creating impossible value estimates</li>
                    <li><strong>With Masking:</strong> Q-values reflect realistic gameplay, only legal action paths considered</li>
                    <li><strong>Performance Gain:</strong> ~15-20% improvement in win rate after masking implementation</li>
                </ul>
                
                <p><strong>Implementation:</strong></p>
                <pre>masked_q = q_values.clone()
masked_q[~legal_mask] = -1e9  # Impossible value
best_action = masked_q.argmax(dim=1)  # Guaranteed legal
target_q = target_network(next_states)[best_action]</pre>
                
                <h3>4. Canonicalization for Sample Efficiency</h3>
                <div class="success">
                    <strong>Achievement:</strong> Canonicalization (always showing board from agent perspective) doubles effective sample efficiency.
                </div>
                
                <p><strong>Mechanism:</strong></p>
                <ul>
                    <li>Network always sees itself as positive pieces (Player 1)</li>
                    <li>Opponent board states rotated 180¬∞ + piece IDs swapped</li>
                    <li>Single strategy learned, applies to both players</li>
                </ul>
                
                <p><strong>Benefit:</strong> 100K episodes of training ‚âà 200K canonical experiences, achieving higher win rates at lower episode counts.</p>
                
                <h3>5. Hardware Optimization Insights</h3>
                <p><strong>GPU vs CPU Memory Trade-off:</strong></p>
                <ul>
                    <li><strong>Full Buffer on GPU:</strong> Requires ~6-8GB VRAM for 100K transitions (RTX 2060 limit)</li>
                    <li><strong>CPU Buffer Strategy:</strong> Store 100K transitions in system RAM (~400MB), transfer only sampling batch to GPU</li>
                    <li><strong>Result:</strong> 90% VRAM savings, enabling larger batch sizes without OOM errors</li>
                </ul>
                
                <h3>6. Discount Factor Impact</h3>
                <p><strong>Empirical Finding:</strong> Discount factor Œ≥ affects convergence and long-term strategy:</p>
                <ul>
                    <li><strong>Œ≥=0.95:</strong> Fast convergence, myopic strategies (5-move horizon)</li>
                    <li><strong>Œ≥=0.99:</strong> Balanced, ~100-move planning horizon</li>
                    <li><strong>Œ≥=0.995:</strong> Long-term optimization, slower convergence, better endgame play</li>
                </ul>
                
                <h3>7. The "Random-Agent Trap"</h3>
                <div class="warning">
                    <strong>Critical Insight:</strong> Agents trained against Random opponents achieve high win rates (98%) but learn brittle strategies.
                </div>
                
                <p><strong>Mechanism:</strong></p>
                <ul>
                    <li>Random agents make blunders (giving up pieces) frequently.</li>
                    <li>The D3QN agent learns: "If I wait, the opponent will eventually kill themselves."</li>
                    <li><strong>Result:</strong> The agent becomes a "Counter-Puncher" that cannot initiate attacks. When playing against another D3QN agent that *doesn't* blunder, it doesn't know how to force a win.</li>
                </ul>
                
                <h3>8. The Zero-Reward Stagnation</h3>
                <p><strong>Discovery:</strong> Our reward function set <code>Reward(Draw) = 0.0</code> and <code>Reward(Loss) = -1.0</code>.</p>
                <ul>
                    <li><strong>Behavior:</strong> In complex endgames, the agent calculated that trying to win carried a risk of losing (Reward -1.0).</li>
                    <li><strong>Optimization:</strong> It realized that stalling for a draw (Reward 0.0) was strictly safer than risking a 49% chance of winning vs 51% chance of losing.</li>
                    <li><strong>Outcome:</strong> Massive draw rates in Gen 10-12 tournaments. The agents refused to take risks.</li>
                </ul>
                
                <h3>9. Self-Play Local Optima</h3>
                <div class="info">
                    <strong>Lesson:</strong> "Iron sharpening Iron" only works if the iron is already hard.
                </div>
                
                <p><strong>Observation:</strong> We populated our Self-Play Opponent Pool with Gen 1-9 models (which were "Random-Killers").</p>
                <ul>
                    <li>Gen 10+ learned to beat Gen 9.</li>
                    <li>Gen 11 learned to beat Gen 10.</li>
                    <li>But Gen 9 was fundamentally flawed (see point 7).</li>
                    <li><strong>Conclusion:</strong> We were optimizing for a flawed metric. We improved at beating bad agents, not at playing Checkers.</li>
                </ul>
            </section>
            
            <!-- INFRASTRUCTURE SECTION -->
            <section id="infrastructure">
                <h2>‚öôÔ∏è Training Infrastructure & Components</h2>
                
                <h3>System Architecture</h3>
                <p>The training system consists of five integrated layers:</p>
                
                <div class="generation-block gen7">
                    <div class="generation-title">Layer 1: Game Environment</div>
                    <p><strong>Classes:</strong> CheckersBoard, CheckersRules, CheckersEnv</p>
                    <p><strong>Responsibilities:</strong></p>
                    <ul>
                        <li>Board state management (8√ó8 with pieces and kings)</li>
                        <li>Legal move generation (simple moves + mandatory captures)</li>
                        <li>Game outcome detection (win/loss/draw)</li>
                        <li>Reward calculation based on outcome and in-game events</li>
                    </ul>
                    <p><strong>Key Features:</strong></p>
                    <ul>
                        <li>Supports arbitrary players (Player 1, Player -1)</li>
                        <li>Automatic king promotion at backline</li>
                        <li>Mandatory capture enforcement</li>
                        <li>Dense reward shaping (captures, material, positional)</li>
                    </ul>
                </div>
                
                <div class="generation-block gen7">
                    <div class="generation-title">Layer 2: Agent Interfaces</div>
                    <p><strong>Classes:</strong> RandomAgent, D3QNAgent</p>
                    <p><strong>Responsibilities:</strong></p>
                    <ul>
                        <li>Consistent action selection interface</li>
                        <li>Legal move filtering and safety checks</li>
                        <li>Checkpoint loading and model inference</li>
                    </ul>
                    <p><strong>Action Selection:</strong></p>
                    <ul>
                        <li><strong>RandomAgent:</strong> Selects uniformly from legal moves.</li>
                        <li><strong>D3QNAgent:</strong> Uses epsilon-greedy strategy (Deterministic when Œµ=0, Stochastic exploration when Œµ>0) with legal action masking.</li>
                    </ul>
                </div>
                
                <div class="generation-block gen7">
                    <div class="generation-title">Layer 3: Common Utilities</div>
                    <p><strong>Classes:</strong> ActionManager, BoardEncoder, MoveParser, ReplayBuffer</p>
                    
                    <p><strong>ActionManager (168 discrete actions):</strong></p>
                    <ul>
                        <li>Maps moves: (from_r, from_c, to_r, to_c) ‚Üî action_id (0-167)</li>
                        <li>Generates legal action masks for batch processing</li>
                        <li>Handles multi-jump captures as single actions</li>
                    </ul>
                    
                    <p><strong>BoardEncoder (5-channel CNN input):</strong></p>
                    <ul>
                        <li>Channel 0: Own regular pieces</li>
                        <li>Channel 1: Own kings</li>
                        <li>Channel 2: Opponent regular pieces</li>
                        <li>Channel 3: Opponent kings</li>
                        <li>Channel 4: Tempo (whose turn)</li>
                        <li>Canonicalization: Always views from Player 1 perspective</li>
                    </ul>
                    
                    <p><strong>ReplayBuffer (CPU-optimized):</strong></p>
                    <ul>
                        <li>Capacity: 100,000 transitions</li>
                        <li>Storage: NumPy arrays in system RAM (~267 MB)</li>
                        <li>Stores: (state, action, reward, next_state, done, next_legal_mask)</li>
                        <li>Circular buffer with automatic overwriting</li>
                        <li>Batch sampling with GPU transfer on-demand</li>
                    </ul>
                </div>
                
                <div class="generation-block gen7">
                    <div class="generation-title">Layer 4: Neural Network</div>
                    <p><strong>Classes:</strong> DuelingDQN, D3QNModel</p>
                    <p><strong>DuelingDQN: Feature Extraction + Dueling Streams</strong></p>
                    <ul>
                        <li>3 convolutional layers (5‚Üí32‚Üí64‚Üí64)</li>
                        <li>LayerNorm for feature normalization</li>
                        <li>Value stream: outputs single scalar V(s)</li>
                        <li>Advantage stream: outputs 168 action advantages A(s,a)</li>
                        <li>Aggregation: Q(s,a) = V(s) + A(s,a) - mean(A)</li>
                    </ul>
                    <p><strong>D3QNModel: Online + Target Networks</strong></p>
                    <ul>
                        <li>Maintains two DuelingDQN instances (online and target)</li>
                        <li>Online updated via gradient descent every step</li>
                        <li>Target updated via Polyak averaging: Œ∏_target = (1-œÑ)√óŒ∏_target + œÑ√óŒ∏_online</li>
                        <li>Soft update parameter œÑ = 0.005</li>
                    </ul>
                </div>
                
                <div class="generation-block gen7">
                    <div class="generation-title">Layer 5: Training Loop</div>
                    <p><strong>Class:</strong> D3QNTrainer</p>
                    <p><strong>Responsibilities:</strong></p>
                    <ul>
                        <li>Experience collection with Œµ-greedy exploration</li>
                        <li>Double DQN TD-error computation with legal action masking</li>
                        <li>Gradient computation and optimization</li>
                        <li>Target network updates</li>
                        <li>Statistics tracking and checkpointing</li>
                    </ul>
                    <p><strong>Training Algorithm:</strong></p>
                    <ol>
                        <li>Select action: a = argmax_a' Q_online(s, a') [with Œµ-noise]</li>
                        <li>Execute in environment, get (s', r, done)</li>
                        <li>Store in replay buffer: (s, a, r, s', done, legal_mask_s')</li>
                        <li>If buffer ‚â• min_size: Sample batch B</li>
                        <li>Double DQN target: Compute best action with online network
                            <ul>
                                <li>q_online = online_network(s')</li>
                                <li>best_actions = argmax_a q_online[legal_mask]</li>
                            </ul>
                        </li>
                        <li>Evaluate with target network:
                            <ul>
                                <li>q_target = target_network(s')[best_actions]</li>
                                <li>TD_target = r + Œ≥ √ó q_target √ó (1 - done)</li>
                            </ul>
                        </li>
                        <li>Compute Huber loss between Q_online(s,a) and TD_target</li>
                        <li>Backpropagate with gradient clipping (max_norm=0.1)</li>
                        <li>Soft update target network</li>
                        <li>Log statistics and save checkpoints</li>
                    </ol>
                </div>
                
                <h3>Evaluation & Analysis Tools</h3>
                <table>
                    <tr>
                        <th>Tool</th>
                        <th>Purpose</th>
                        <th>Input</th>
                        <th>Output</th>
                    </tr>
                    <tr>
                        <td>evaluate.py</td>
                        <td>Visual game playback</td>
                        <td>Checkpoint path, player selection</td>
                        <td>Interactive board visualization, Q-values</td>
                    </tr>
                    <tr>
                        <td>benchmark.py</td>
                        <td>Tournament testing</td>
                        <td>All checkpoints in directory</td>
                        <td>Win rates vs random opponent, ranking</td>
                    </tr>
                    <tr>
                        <td>diagnose_rewards.py</td>
                        <td>Reward analysis</td>
                        <td>100 random games</td>
                        <td>Min/max/mean rewards, distribution</td>
                    </tr>
                    <tr>
                        <td>plot_logs.py</td>
                        <td>Training visualization</td>
                        <td>training_log.txt</td>
                        <td>Loss and reward curves over episodes</td>
                    </tr>
                </table>
                
                <h3>Self-Play & Curriculum System (Gen 10+)</h3>
                <p><strong>Opponent Pool Management:</strong></p>
                <ul>
                    <li>Maintain 10 previous generations in memory</li>
                    <li>Each generation gets separate checkpoint file</li>
                    <li>Tournament ranking system for curriculum selection</li>
                </ul>
                
                <p><strong>Training Schedule:</strong></p>
                <ul>
                    <li>50% games: vs current best model</li>
                    <li>30% games: vs mid-level opponents (N-3 to N-1)</li>
                    <li>20% games: vs strongest archived opponent</li>
                </ul>
                
                <p><strong>Iron League Tournament:</strong></p>
                <ul>
                    <li>Round-robin between all archived generations</li>
                    <li>Each pair: 10 games (5 as red, 5 as black)</li>
                    <li>Rating calculation using Elo formula</li>
                    <li>Enables tracking evolutionary progress</li>
                </ul>
            </section>
            
            <!-- EVALUATION SECTION -->
            <section id="evaluation">
                <h2>üìä Evaluation & Performance Metrics</h2>
                
                <h3>Tournament Ranking (Dec 2025 - Fair Tournament)</h3>
                <p>Results from 480-game round-robin tournament. <span style="color: red; font-weight: bold;">CRITICAL FINDING:</span> Older Gen 7 model dominates newer generations.</p>
                <table>
                    <tr>
                        <th>Rank</th>
                        <th>Agent Name</th>
                        <th>P1 WR</th>
                        <th>P2 WR</th>
                        <th>Overall WR</th>
                        <th>Balance Gap</th>
                    </tr>
                    <tr>
                        <td>1</td>
                        <td>gen7_specialist (The "Old Master")</td>
                        <td>66.7%</td>
                        <td>66.7%</td>
                        <td>66.7%</td>
                        <td>0.0%</td>
                    </tr>
                    <tr>
                        <td>2</td>
                        <td>gen11_decisive_500</td>
                        <td>41.7%</td>
                        <td>58.3%</td>
                        <td>50.0%</td>
                        <td>16.7%</td>
                    </tr>
                    <tr>
                        <td>3</td>
                        <td>gen9_titan_62vT</td>
                        <td>33.3%</td>
                        <td>58.3%</td>
                        <td>45.8%</td>
                        <td>25.0%</td>
                    </tr>
                    <tr>
                        <td>4</td>
                        <td>gen9_champion_58vT</td>
                        <td>25.0%</td>
                        <td>66.7%</td>
                        <td>45.8%</td>
                        <td>41.7%</td>
                    </tr>
                    <tr>
                        <td>5</td>
                        <td>gen12_elite_3500</td>
                        <td>33.3%</td>
                        <td>33.3%</td>
                        <td>33.3%</td>
                        <td>0.0%</td>
                    </tr>
                    <tr>
                        <td>6</td>
                        <td>gen12_elite_2500</td>
                        <td>33.3%</td>
                        <td>25.0%</td>
                        <td>29.2%</td>
                        <td>8.3%</td>
                    </tr>
                    <tr>
                        <td>7</td>
                        <td>gen12_elite_2000</td>
                        <td>41.7%</td>
                        <td>8.3%</td>
                        <td>25.0%</td>
                        <td>33.3%</td>
                    </tr>
                    <tr>
                        <td>8</td>
                        <td>gen12_elite_3000</td>
                        <td>25.0%</td>
                        <td>25.0%</td>
                        <td>25.0%</td>
                        <td>0.0%</td>
                    </tr>
                    <tr>
                        <td>9</td>
                        <td>gen8_titan_LEGACY</td>
                        <td>33.3%</td>
                        <td>8.3%</td>
                        <td>20.8%</td>
                        <td>25.0%</td>
                    </tr>
                    <tr>
                        <td>10</td>
                        <td>gen12_elite_1500</td>
                        <td>8.3%</td>
                        <td>33.3%</td>
                        <td>20.8%</td>
                        <td>25.0%</td>
                    </tr>
                    <tr>
                        <td>11</td>
                        <td>gen12_elite_1000</td>
                        <td>25.0%</td>
                        <td>16.7%</td>
                        <td>20.8%</td>
                        <td>8.3%</td>
                    </tr>
                    <tr>
                        <td>12</td>
                        <td>gen12_elite_500</td>
                        <td>16.7%</td>
                        <td>16.7%</td>
                        <td>16.7%</td>
                        <td>0.0%</td>
                    </tr>
                    <tr>
                        <td>13</td>
                        <td>glass_cannon (Experimental)</td>
                        <td>0.0%</td>
                        <td>25.0%</td>
                        <td>12.5%</td>
                        <td>25.0%</td>
                    </tr>
                </table>
                
                <div class="warning" style="border-left-color: #d63384; background-color: #fff0f6;">
                    <strong>üö® CRITICAL REGRESSION DETECTED</strong>
                    <p>The latest tournament results reveal a shocking regression in the project's evolution. <strong>Gen 7 Specialist</strong> (an older model trained with simple normalized rewards) is crushing the sophisticated Gen 10-12 agents.</p>
                    <p><strong>Why is Gen 7 winning?</strong></p>
                    <ul>
                        <li><strong>Simplicity:</strong> It uses a pure Win/Loss reward structure without complex "curriculum" or "endgame injection" noise.</li>
                        <li><strong>Overfitting in Later Gens:</strong> Gen 10+ models were trained against specific "exploiters" (like Gen 9), creating a rock-paper-scissors dynamic where they forgot how to play fundamental checkers. Gen 7 plays solid, fundamental checkers.</li>
                        <li><strong>The "Teacher" Problem:</strong> Later generations were "taught" by agents that had already learned bad habits (passive play, stalling). Gen 7 was trained before these bad habits solidified.</li>
                    </ul>
                    <p><strong>Conclusion:</strong> We must revert to the Gen 7 reward philosophy and abandon the current "Self-Play Curriculum" until we integrate a true external expert.</p>
                </div>

                <div class="card" style="margin-top: 40px; background: white; border: none; box-shadow: 0 10px 30px rgba(0,0,0,0.1);">
                    <div style="padding: 20px; border-bottom: 1px solid #eee;">
                        <h4 style="margin: 0; color: #333;">üìä Agent Performance Analysis</h4>
                        <p style="margin: 5px 0 0; color: #666; font-size: 0.9em;">Win Rate Comparison: Player 1 (Red) vs Player 2 (Black)</p>
                    </div>
                    <img src="tournament_plot.png" alt="Tournament Results Plot" style="display: block; width: 100%; height: auto;">
                </div>
                
                <h3>Learning Curves</h3>
                <p><strong>Gen 7 (Final Standard Model) - 20,000 Episodes:</strong></p>
                <ul>
                    <li><strong>Episodes 0-2,000:</strong> Learning from scratch (40% ‚Üí 80%)</li>
                    <li><strong>Episodes 2,000-5,000:</strong> Rapid improvement (80% ‚Üí 90%)</li>
                    <li><strong>Episodes 5,000-10,000:</strong> Convergence (90% ‚Üí 93%)</li>
                    <li><strong>Episodes 10,000-20,000:</strong> Plateau & consolidation (93% ‚Üí 95%)</li>
                </ul>
                
                <p><strong>Loss Curve Profile:</strong></p>
                <ul>
                    <li><strong>Initial (Ep. 0-1,000):</strong> Exponential decay 0.8 ‚Üí 0.3</li>
                    <li><strong>Acceleration (Ep. 1,000-5,000):</strong> Fast decay 0.3 ‚Üí 0.05</li>
                    <li><strong>Saturation (Ep. 5,000+):</strong> Slow oscillation 0.05 ¬± 0.01</li>
                </ul>
                
                <h3>Game Outcome Analysis</h3>
                <div class="grid">
                    <div class="card">
                        <h4>Wins Distribution</h4>
                        <p><strong>Early Game Wins (30%)</strong> - Captures and aggressive tactics</p>
                        <p><strong>Mid Game Wins (45%)</strong> - King promotion advantage</p>
                        <p><strong>End Game Wins (25%)</strong> - Positional superiority</p>
                    </div>
                    <div class="card">
                        <h4>Loss Analysis</h4>
                        <p><strong>Material Loss (60%)</strong> - Outmaneuvered in capture chains</p>
                        <p><strong>Position Loss (30%)</strong> - Trapped pieces or zugzwang</p>
                        <p><strong>Rare Loss (10%)</strong> - Early game blunders</p>
                    </div>
                </div>
                
                <h3>Self-Play Evolution (Gen 10)</h3>
                <table>
                    <tr>
                        <th>Generation Pair</th>
                        <th>Gen N vs Gen N-1</th>
                        <th>Improvement</th>
                        <th>Novel Strategies</th>
                    </tr>
                    <tr>
                        <td>Gen 10.0 vs 9.x</td>
                        <td>65% wins</td>
                        <td>+7% win rate</td>
                        <td>Aggressive king trading</td>
                    </tr>
                    <tr>
                        <td>Gen 10.1 vs 10.0</td>
                        <td>58% wins</td>
                        <td>+2% win rate</td>
                        <td>Anti-aggressive defense</td>
                    </tr>
                    <tr>
                        <td>Gen 10.2 vs 10.1</td>
                        <td>52% wins</td>
                        <td>+0.5% win rate</td>
                        <td>Positional maneuvering</td>
                    </tr>
                </table>
                
                <p><strong>Interpretation:</strong> Decreasing win margins indicate convergence to local Nash equilibrium, where each generation learns specialized counters.</p>
            </section>
            
            <!-- CONCLUSION SECTION -->
            <section id="conclusion">
                <h2>üéì Conclusions & Critical Retrospective</h2>
                
                <h3>Project Reality Check</h3>
                <div class="grid">
                    <div class="card">
                        <h4>‚úÖ Infrastructure Success</h4>
                        <p>We built a robust, stable D3QN training pipeline that solved numerical instability and reward hacking. The <em>tool</em> works perfectly.</p>
                    </div>
                    <div class="card">
                        <h4>‚ùå Model Regression</h4>
                        <p>Contrary to expectations, <strong>Gen 7 (the simple baseline) outperforms Gen 12 (the complex iteration).</strong> Adding complexity (curriculum, specialized rewards) degraded performance.</p>
                    </div>
                    <div class="card">
                        <h4>‚ö†Ô∏è The "Teacher" Trap</h4>
                        <p>Self-play training failed because the agents learned to exploit each other's specific weaknesses (like passive play) rather than learning general checkers strategy.</p>
                    </div>
                    <div class="card">
                        <h4>‚ö†Ô∏è The D3QN Ceiling</h4>
                        <p>Pure value-based methods (DQN) struggle with the long-term planning required for Checkers endgames without an explicit search lookahead.</p>
                    </div>
                </div>
                
                <h3>Critical Analysis: Why We Failed to Improve</h3>
                <p>Despite the advanced architecture in Gen 10, 11, and 12, performance did not just stagnate‚Äîit <strong>regressed</strong>. Our analysis identifies three critical bottlenecks:</p>
                
                <ol>
                    <li><strong>Loss of Fundamentals:</strong> 
                        <br>Gen 7 learned "Checkers Fundamentals" (King safety, center control). Later generations learned "Meta-Gaming" (how to trick Gen 10 into a draw). When faced with a solid opponent (Gen 7), the "tricky" agents collapsed.
                    </li>
                    <li><strong>Reward Shaping Backfire:</strong> 
                        <br>The aggressive "Living Tax" in Gen 11/12 was intended to force action. Instead, it likely caused agents to make reckless moves just to avoid the penalty, leading to material loss against a patient opponent.
                    </li>
                    <li><strong>Blind Optimization:</strong> 
                        <br>We optimized for "Win Rate vs Previous Generation". This is a relative metric. We never checked if the "Previous Generation" was actually playing *good* checkers, or just *different* checkers.
                    </li>
                </ol>

                <h3>Final Remarks</h3>
                <p>This project was a triumph of <strong>Software Engineering</strong> but a cautionary tale in <strong>Reinforcement Learning</strong>. We successfully built the car, but we drove it into a ditch by trusting self-play without a compass.</p>
                
                <div class="info">
                    <strong>Verdict:</strong> The project proves that <strong>Architecture < Curriculum</strong>. A simple model (Gen 7) with a clear objective beats a complex model (Gen 12) with a confused objective. The next phase must focus on <strong>Quality of Experience</strong> rather than Quantity of Training.
                </div>
            </section>
            
        </div>
        
        <footer>
            <p><strong>Checkers ML: D3QN Complete Documentation</strong></p>
            <p>Project by Anas Mtaweh | Location: Beirut, Lebanon</p>
            <p>Repository: <a href="https://github.com/Anasmtaweh/Checkers-Ml_D3QN_MCTSn.git" target="_blank">GitHub - Checkers ML D3QN</a></p>
            <p style="margin-top: 20px; font-size: 0.95em; opacity: 0.8;">
                Generated: December 26, 2025 | Hardware: RTX 2060 + i7 10th Gen | Framework: PyTorch
            </p>
        </footer>
    </div>
</body>
</html>
