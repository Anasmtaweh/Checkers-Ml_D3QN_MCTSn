
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                PHASE 3 COMPLETE: TRAINING INFRASTRUCTURE                  â•‘
â•‘                   D3QN Checkers Reinforcement Learning                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PROJECT STATUS: Phase 1 âœ“ | Phase 2 âœ“ | Phase 3 âœ“ COMPLETE | Ready for Training

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ COMPLETE PROJECT STRUCTURE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

checkers_project/
â”‚
â”œâ”€â”€ checkers_env/                    [Game Logic - Existing]
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ env.py                       âœ“ CheckersEnv with reward shaping
â”‚   â”œâ”€â”€ board.py                     âœ“ CheckersBoard state management
â”‚   â””â”€â”€ rules.py                     âœ“ CheckersRules move generation
â”‚
â”œâ”€â”€ checkers_agents/                 [Agents - Existing]
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ d3qn_agent.py               âœ“ D3QNAgent for inference
â”‚   â”œâ”€â”€ random_agent.py             âœ“ Baseline random agents
â”‚   â””â”€â”€ mcts_agent.py               âœ“ Placeholder for MCTS
â”‚
â””â”€â”€ training/                        [Training System - NEW]
    â”œâ”€â”€ __init__.py                  âœ“ Module initialization
    â”‚
    â”œâ”€â”€ common/                      âœ“ PHASE 1 + 3
    â”‚   â”œâ”€â”€ __init__.py              âœ“ Exports all utilities
    â”‚   â”œâ”€â”€ action_manager.py        âœ“ Universal action space
    â”‚   â”œâ”€â”€ board_encoder.py         âœ“ 5-channel encoding
    â”‚   â”œâ”€â”€ move_parser.py           âœ“ Move format utilities
    â”‚   â””â”€â”€ buffer.py                âœ“ NEW: Replay buffer (Phase 3)
    â”‚
    â””â”€â”€ d3qn/                        âœ“ PHASE 2 + 3
        â”œâ”€â”€ __init__.py              âœ“ Exports model and trainer
        â”œâ”€â”€ model.py                 âœ“ Dueling DQN architecture
        â””â”€â”€ trainer.py               âœ“ NEW: Training loop (Phase 3)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ†• PHASE 3 IMPLEMENTATIONS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. ReplayBuffer (training/common/buffer.py)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PURPOSE:
  Efficiently store and sample experience transitions for training.

KEY FEATURES:
  âœ“ Pre-allocated numpy arrays (optimized for RTX 2060 + 24GB RAM)
  âœ“ Circular buffer (overwrites oldest when full)
  âœ“ CPU storage, GPU transfer only during sampling (saves VRAM)
  âœ“ Stores legal action masks for next states (crucial for Double DQN)
  âœ“ Type-safe (float32 for states/rewards, int64 for actions, bool for masks)

STORAGE ARRAYS:
  â€¢ states:       (capacity, 5, 8, 8)  float32  - Current states
  â€¢ next_states:  (capacity, 5, 8, 8)  float32  - Resulting states
  â€¢ actions:      (capacity, 1)        int64    - Action indices taken
  â€¢ rewards:      (capacity, 1)        float32  - Immediate rewards
  â€¢ dones:        (capacity, 1)        bool     - Episode termination flags
  â€¢ legal_masks:  (capacity, action_dim) bool   - Legal actions in next state

MEMORY USAGE:
  Capacity 100,000:
    States:       100K Ã— 5 Ã— 8 Ã— 8 Ã— 4 bytes = ~125 MB
    Next states:  100K Ã— 5 Ã— 8 Ã— 8 Ã— 4 bytes = ~125 MB
    Actions:      100K Ã— 1 Ã— 8 bytes         = ~0.8 MB
    Rewards:      100K Ã— 1 Ã— 4 bytes         = ~0.4 MB
    Dones:        100K Ã— 1 Ã— 1 byte          = ~0.1 MB
    Legal masks:  100K Ã— 168 Ã— 1 byte        = ~16 MB
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    TOTAL:                                   â‰ˆ 267 MB

METHODS:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ push(state, action, reward, next_state, done, mask)          â”‚
  â”‚   Add transition to buffer (circular overwrite)              â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ sample(batch_size) -> (states, actions, rewards, ...)        â”‚
  â”‚   Randomly sample batch and convert to GPU tensors           â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ __len__() -> int                                             â”‚
  â”‚   Return current number of stored transitions                â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ is_ready(min_size) -> bool                                   â”‚
  â”‚   Check if enough samples for training                       â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ save(path) / load(path)                                      â”‚
  â”‚   Save/load buffer to/from disk (.npz format)                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

USAGE EXAMPLE:
  ```python
  buffer = ReplayBuffer(capacity=100000, action_dim=168, device="cuda")

  # Store transition
  buffer.push(state, action_id, reward, next_state, done, next_mask)

  # Sample for training
  if len(buffer) >= batch_size:
      batch = buffer.sample(batch_size=64)
      states, actions, rewards, next_states, dones, masks = batch
  ```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

2. D3QNTrainer (training/d3qn/trainer.py)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PURPOSE:
  Orchestrate the complete training process for D3QN.

KEY FEATURES:
  âœ“ Double DQN implementation (reduces Q-value overestimation)
  âœ“ Legal action masking in target computation
  âœ“ Gradient clipping (prevents exploding gradients)
  âœ“ Flexible loss functions (Huber or MSE)
  âœ“ Experience collection with epsilon-greedy
  âœ“ Statistics tracking and checkpointing

CONSTRUCTOR:
  ```python
  trainer = D3QNTrainer(
      env=checkers_env,
      action_manager=action_mgr,
      board_encoder=encoder,
      model=d3qn_model,
      optimizer=optimizer,
      buffer=replay_buffer,
      device="cuda",
      gamma=0.99,
      gradient_clip=1.0,
      loss_type="huber"
  )
  ```

CORE METHODS:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ train_step(batch_size) -> float                              â”‚
  â”‚   Perform one training iteration using Double DQN            â”‚
  â”‚   Returns: loss value                                        â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ update_target_network()                                      â”‚
  â”‚   Hard copy: online weights â†’ target weights                â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ collect_experience(num_steps, epsilon, render=False)         â”‚
  â”‚   Play in environment and store transitions in buffer        â”‚
  â”‚   Returns: statistics dict                                   â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ get_statistics(window=100) -> dict                           â”‚
  â”‚   Get training metrics (rewards, losses, etc.)               â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ save_checkpoint(path) / load_checkpoint(path)                â”‚
  â”‚   Save/load complete training state                          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¬ DOUBLE DQN ALGORITHM (train_step)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

STEP-BY-STEP BREAKDOWN:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Sample Batch from Replay Buffer                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ states, actions, rewards, next_states, dones, next_masks =         â”‚
â”‚     buffer.sample(batch_size)                                      â”‚
â”‚                                                                     â”‚
â”‚ Shapes:                                                             â”‚
â”‚   states:      (batch, 5, 8, 8)                                    â”‚
â”‚   actions:     (batch, 1)         # Actions that were taken        â”‚
â”‚   rewards:     (batch, 1)         # Immediate rewards              â”‚
â”‚   next_states: (batch, 5, 8, 8)                                    â”‚
â”‚   dones:       (batch, 1)         # Episode termination flags      â”‚
â”‚   next_masks:  (batch, action_dim) # Legal actions in next states  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: Compute Current Q-Values                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ current_q_values = model.online(states)  # (batch, action_dim)     â”‚
â”‚ current_q = current_q_values.gather(1, actions)  # (batch, 1)      â”‚
â”‚                                                                     â”‚
â”‚ This gets Q(s, a) for the actions that were actually taken.        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Compute Target Q-Values (Double DQN with Legal Masking)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ with torch.no_grad():                                               â”‚
â”‚     # 3a) Online network proposes best actions                     â”‚
â”‚     next_q_online = model.online(next_states)  # (batch, action_dim)â”‚
â”‚                                                                     â”‚
â”‚     # 3b) Apply legal action masks (CRUCIAL!)                      â”‚
â”‚     masked_next_q_online = next_q_online.clone()                   â”‚
â”‚     masked_next_q_online[~next_masks] = -1e9                       â”‚
â”‚     # Now illegal actions have very negative Q-values              â”‚
â”‚                                                                     â”‚
â”‚     # 3c) Select best LEGAL action                                 â”‚
â”‚     best_next_actions = masked_next_q_online.argmax(1, keepdim=True)â”‚
â”‚     # Shape: (batch, 1)                                            â”‚
â”‚                                                                     â”‚
â”‚     # 3d) Target network evaluates those actions                   â”‚
â”‚     next_q_target = model.target(next_states)  # (batch, action_dim)â”‚
â”‚     next_q = next_q_target.gather(1, best_next_actions)  # (batch, 1)â”‚
â”‚                                                                     â”‚
â”‚     # 3e) Compute TD target                                        â”‚
â”‚     target_q = rewards + gamma * next_q * (1 - dones)              â”‚
â”‚     # Shape: (batch, 1)                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: Compute Loss                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ if loss_type == "huber":                                            â”‚
â”‚     loss = F.smooth_l1_loss(current_q, target_q)                   â”‚
â”‚ else:                                                               â”‚
â”‚     loss = F.mse_loss(current_q, target_q)                         â”‚
â”‚                                                                     â”‚
â”‚ Huber loss is preferred: smooth near zero, linear for outliers     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 5: Backpropagation                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ optimizer.zero_grad()                                               â”‚
â”‚ loss.backward()                                                     â”‚
â”‚                                                                     â”‚
â”‚ # Gradient clipping (prevents exploding gradients)                 â”‚
â”‚ torch.nn.utils.clip_grad_norm_(model.online.parameters(), 1.0)     â”‚
â”‚                                                                     â”‚
â”‚ optimizer.step()                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ WHY LEGAL ACTION MASKING IN TARGET COMPUTATION?
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

PROBLEM:
  In standard Double DQN, the online network might select an illegal action
  in the next state (e.g., moving to occupied square). If we then evaluate
  this illegal action with the target network, we get meaningless Q-values.

SOLUTION:
  Apply legal action mask BEFORE selecting best action:

  masked_q[~legal_mask] = -1e9  # Make illegal actions unattractive
  best_action = masked_q.argmax()  # Now guaranteed to be legal

IMPACT:
  âœ“ Target values are always computed for legal, executable actions
  âœ“ Network learns realistic value estimates
  âœ“ Prevents training instability from illegal action selection
  âœ“ Critical for games with variable action spaces

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š COMPLETE TRAINING LOOP TEMPLATE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

```python
import torch
import torch.optim as optim
from training.common import ActionManager, CheckersBoardEncoder, ReplayBuffer
from training.d3qn import D3QNModel, D3QNTrainer
from checkers_env import CheckersEnv

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HYPERPARAMETERS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BUFFER_CAPACITY = 100_000
BATCH_SIZE = 64
LEARNING_RATE = 1e-4
GAMMA = 0.99
EPSILON_START = 1.0
EPSILON_END = 0.01
EPSILON_DECAY = 0.9995
TARGET_UPDATE_FREQ = 1000  # steps
TRAINING_STEPS = 100_000
COLLECT_STEPS_PER_ITERATION = 4
MIN_BUFFER_SIZE = 1000

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INITIALIZATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
env = CheckersEnv()
action_manager = ActionManager(device=DEVICE)
encoder = CheckersBoardEncoder()

model = D3QNModel(
    action_dim=action_manager.action_dim,
    device=DEVICE
)

optimizer = optim.Adam(model.online.parameters(), lr=LEARNING_RATE)

buffer = ReplayBuffer(
    capacity=BUFFER_CAPACITY,
    action_dim=action_manager.action_dim,
    device=DEVICE
)

trainer = D3QNTrainer(
    env=env,
    action_manager=action_manager,
    board_encoder=encoder,
    model=model,
    optimizer=optimizer,
    buffer=buffer,
    device=DEVICE,
    gamma=GAMMA,
    gradient_clip=1.0,
    loss_type="huber"
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRAINING LOOP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
epsilon = EPSILON_START

for step in range(TRAINING_STEPS):
    # Collect experience
    trainer.collect_experience(
        num_steps=COLLECT_STEPS_PER_ITERATION,
        epsilon=epsilon
    )

    # Train if buffer has enough samples
    if len(buffer) >= MIN_BUFFER_SIZE:
        loss = trainer.train_step(BATCH_SIZE)

        # Update target network periodically
        if step % TARGET_UPDATE_FREQ == 0:
            trainer.update_target_network()
            print(f"Step {step}: Target network updated")

        # Decay epsilon
        epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)

    # Logging
    if step % 100 == 0:
        stats = trainer.get_statistics(window=100)
        print(f"Step {step:6d} | "
              f"Loss: {stats.get('avg_loss', 0):.4f} | "
              f"Reward: {stats.get('avg_reward', 0):7.2f} | "
              f"Eps: {epsilon:.3f} | "
              f"Buffer: {len(buffer):6d}")

    # Save checkpoint
    if step % 10000 == 0 and step > 0:
        trainer.save_checkpoint(f"checkpoint_step_{step}.pt")

print("Training complete!")
```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš™ï¸ RECOMMENDED HYPERPARAMETERS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Hardware: RTX 2060 (6GB VRAM) + 24GB RAM
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

BUFFER SETTINGS:
  â€¢ Capacity: 100,000 - 200,000 transitions
    - Larger = better sample diversity
    - 100K uses ~267 MB RAM (very manageable)

  â€¢ Min size before training: 1,000 - 5,000
    - Ensures initial exploration

TRAINING SETTINGS:
  â€¢ Batch size: 64 - 128
    - Larger = more stable gradients, slower
    - 64 is good balance for RTX 2060

  â€¢ Learning rate: 1e-4 - 3e-4
    - Start with 1e-4
    - Can use learning rate scheduling

  â€¢ Gamma (discount): 0.99
    - High value for long-term planning in checkers

  â€¢ Gradient clip: 1.0
    - Prevents exploding gradients

EXPLORATION:
  â€¢ Epsilon start: 1.0 (100% random initially)
  â€¢ Epsilon end: 0.01 - 0.05 (1-5% random at convergence)
  â€¢ Epsilon decay: 0.9995 - 0.999
    - Reaches ~0.05 after 10K-30K steps

TARGET NETWORK:
  â€¢ Update frequency: 1,000 - 10,000 steps
    - More frequent = faster adaptation but less stable
    - 1,000 is a good starting point

LOSS FUNCTION:
  â€¢ Huber loss (smooth L1) - RECOMMENDED
    - More robust to outliers than MSE
    - Smoother gradients

OPTIMIZER:
  â€¢ Adam with default betas (0.9, 0.999)
    - Adaptive learning rates
    - Works well for RL

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸš€ TRAINING WORKFLOW
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

PHASE 1: Initial Exploration (Steps 0 - 10K)
  â€¢ High epsilon (1.0 â†’ 0.5)
  â€¢ Build replay buffer
  â€¢ Network learns basic patterns
  â€¢ Expected: Negative rewards, random play

PHASE 2: Learning (Steps 10K - 50K)
  â€¢ Decreasing epsilon (0.5 â†’ 0.1)
  â€¢ Network starts making reasonable moves
  â€¢ Reward increases
  â€¢ Expected: Win rate improves vs random

PHASE 3: Refinement (Steps 50K - 100K+)
  â€¢ Low epsilon (0.1 â†’ 0.01)
  â€¢ Network refines strategy
  â€¢ Expected: Consistent wins vs random opponent

MONITORING:
  âœ“ Average reward (should increase)
  âœ“ Average episode length (may increase initially as agent learns to play longer)
  âœ“ Loss value (should stabilize, not diverge)
  âœ“ Win rate vs random opponent (periodic evaluation)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ˆ EXPECTED TRAINING CURVES
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Average Reward:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     50 â”‚                                       â•±â•²
        â”‚                                    â•±â•²â•±  â•²
     25 â”‚                               â•±â•²â•±â•²â•±      
        â”‚                          â•±â•²â•±â•²â•±            
      0 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±â•²â•±â•²â•±â•²â•±                 
        â”‚             â•±â•²â•±â•²â•±                        
    -25 â”‚        â•±â•²â•±â•²â•±                             
        â”‚   â•±â•²â•±â•²â•±                                  
    -50 â”‚â•±â•²â•±                                       
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          0    20K   40K   60K   80K   100K steps

Loss:
â”€â”€â”€â”€â”€
    2.0 â”‚â•²                                        
        â”‚ â•²â•²                                      
    1.5 â”‚  â•²â•²â•²                                    
        â”‚    â•²â•²â•²                                  
    1.0 â”‚      â•²â•²â•²â•²â•²                              
        â”‚          â•²â•²â•²â•²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    0.5 â”‚              â•²â•²â•²â•²                       
        â”‚                 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          0    20K   40K   60K   80K   100K steps

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”§ TROUBLESHOOTING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

PROBLEM: Loss exploding (going to infinity)
SOLUTION:
  âœ“ Reduce learning rate (try 3e-5)
  âœ“ Increase gradient clipping (try 0.5)
  âœ“ Check for NaN in Q-values
  âœ“ Ensure legal masks are applied correctly

PROBLEM: Reward not improving
SOLUTION:
  âœ“ Check epsilon decay (might be too slow)
  âœ“ Increase buffer size
  âœ“ Verify reward shaping in environment
  âœ“ Try different target update frequency

PROBLEM: Training very slow
SOLUTION:
  âœ“ Reduce batch size
  âœ“ Reduce buffer capacity
  âœ“ Ensure CUDA is being used (check device)
  âœ“ Profile code for bottlenecks

PROBLEM: Agent makes illegal moves during inference
SOLUTION:
  âœ“ Always apply legal mask before argmax
  âœ“ Check mask generation in ActionManager
  âœ“ Verify move mapping is correct

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… PHASE 3 CHECKLIST
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Implementation:
  âœ“ ReplayBuffer class
    âœ“ Pre-allocated numpy arrays
    âœ“ Circular buffer logic
    âœ“ CPU storage, GPU sampling
    âœ“ Legal mask storage for next states
    âœ“ save/load functionality

  âœ“ D3QNTrainer class
    âœ“ Double DQN algorithm
    âœ“ Legal action masking in target computation
    âœ“ train_step() method
    âœ“ update_target_network() method
    âœ“ collect_experience() method
    âœ“ Statistics tracking
    âœ“ Checkpointing

Integration:
  âœ“ Compatible with Phase 1 (ActionManager, BoardEncoder)
  âœ“ Compatible with Phase 2 (D3QNModel)
  âœ“ Compatible with existing environment (CheckersEnv)
  âœ“ Type-safe (float32, int64, bool)

Documentation:
  âœ“ Algorithm explanation
  âœ“ Usage examples
  âœ“ Hyperparameter recommendations
  âœ“ Training workflow
  âœ“ Troubleshooting guide

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ NEXT STEPS: START TRAINING!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Your complete D3QN training system is ready. To start training:

1. Copy the training loop template from this document
2. Adjust hyperparameters for your hardware
3. Run training script
4. Monitor progress (loss, rewards, win rate)
5. Save checkpoints periodically
6. Evaluate agent performance vs baselines

Optional Enhancements (Future):
  â–¡ Prioritized Experience Replay (PER)
  â–¡ Noisy Networks for exploration
  â–¡ Multi-step returns (n-step TD)
  â–¡ Rainbow DQN (combines multiple improvements)
  â–¡ Self-play with opponent pool
  â–¡ TensorBoard logging
  â–¡ Learning rate scheduling
  â–¡ Dueling network ablation studies

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ALL PHASES COMPLETE âœ“âœ“âœ“

You now have a production-ready D3QN system for learning to play checkers!

Generated: December 18, 2025, 2:45 AM EET
Project: Checkers D3QN Reinforcement Learning
Hardware: RTX 2060 + 24GB RAM
Developer: ML Engineer (ml gen 2 Space)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
