# Checkers AI with D3QN - Complete Project Documentation

## Project Overview

This is a comprehensive **Checkers AI training system** using **Dueling Double Deep Q-Network (D3QN)** with reinforcement learning. The agent learns to play checkers through self-play against a random opponent, with support for training resumption, checkpoint management, and performance benchmarking.[1][2][3]

## Project Structure

### Directory Organization

```
checkers_project/
├── checkers_env/          # Game environment
│   ├── board.py
│   ├── rules.py
│   ├── env.py
│   └── __init__.py
├── checkers_agents/       # Agent implementations
│   ├── random_agent.py
│   ├── d3qn_agent.py
│   ├── mcts_agent.py (stub)
│   └── __init__.py
├── training/              # Training infrastructure
│   ├── common/           # Shared utilities
│   │   ├── action_manager.py
│   │   ├── board_encoder.py
│   │   ├── move_parser.py
│   │   ├── buffer.py
│   │   └── __init__.py
│   └── d3qn/            # D3QN implementation
│       ├── model.py
│       ├── trainer.py
│       └── __init__.py
├── main.py               # Primary training script
├── train_d3qn_checkers.py
├── evaluate.py           # Model evaluation
├── benchmark.py          # Performance testing
├── diagnose_rewards.py   # Reward debugging
└── plot_logs.py         # Training visualization
```

## Core Components

### 1. **Checkers Environment** (`checkers_env/`)

#### **board.py**[4]
- **Class**: `CheckersBoard`
- **Purpose**: Manages the 8×8 checkers board state
- **Board Representation**: NumPy array where:
  - `1` = Player 1 piece (red)
  - `-1` = Player -1 piece (black)
  - `2` = Player 1 king
  - `-2` = Player -1 king
  - `0` = Empty square
- **Key Methods**:
  - `reset()`: Initialize standard checkers starting position
  - `get_state()`: Return copy of current board
  - `move_piece(r1, c1, r2, c2)`: Execute move with automatic king promotion
  - `print_board()`: Console display

#### **rules.py**[5]
- **Class**: `CheckersRules`
- **Purpose**: Implements official checkers rules including mandatory captures
- **Key Methods**:
  - `simple_moves(board, player)`: Generate non-capturing moves
  - `capture_sequences(board, player, start_pos)`: Depth-first search for all capture chains (multi-jumps)
  - `get_legal_moves(board, player, forced_from)`: Returns captures if available (mandatory), otherwise simple moves
- **Handles**: Chain captures, king movement, forced continuation moves

#### **env.py**[1]
- **Class**: `CheckersEnv`
- **Purpose**: OpenAI Gym-style environment with reward shaping
- **Reward Structure**:
  - **Terminal Rewards**: +50 win, -50 loss
  - **Captures**: +10 per piece captured
  - **Multi-jump Bonus**: +4 per additional jump in chain
  - **King Promotion**: +5
  - **Positional Shaping**: +0.5 center control, -0.2 back rank
  - **Safety Penalty**: -0.1 for hanging pieces
  - **Move Cost**: -1 per move (time penalty)
  - **Material Delta**: Tracks piece advantage changes
- **Key Methods**:
  - `reset()`: Start new game
  - `step(action)`: Execute move, return (state, reward, done, info)
  - `legal_moves()`: Get available moves
  - `piece_stats()`: Count pieces and kings per player

### 2. **Agent Implementations** (`checkers_agents/`)

#### **random_agent.py**[6]
- **Classes**: `CheckersRandomAgent`, `RandomAgent`
- **Purpose**: Baseline opponent that selects random legal moves
- **Usage**: Training opponent and performance benchmarking

#### **d3qn_agent.py**[7]
- **Class**: `D3QNAgent`
- **Purpose**: Inference-only D3QN agent for gameplay
- **Features**:
  - Epsilon-greedy action selection
  - Multiple checkpoint format support
  - Legal move masking
  - NaN/Inf detection with fallback
- **Key Methods**:
  - `load_weights(path)`: Load trained model
  - `select_action(board, player, legal_moves, epsilon)`: Choose action using Q-values

#### **mcts_agent.py**[4]
- **Status**: Placeholder (not implemented)

### 3. **Training Infrastructure** (`training/`)

#### **Common Utilities** (`training/common/`)

##### **action_manager.py**[8]
- **Class**: `ActionManager`
- **Purpose**: Converts between checkers moves and neural network action indices
- **Action Space**: 168 discrete actions (32 starting positions × 5-6 possible destinations)
- **Key Methods**:
  - `get_action_id(move)`: Convert move to integer ID
  - `get_move_from_id(action_id)`: Convert ID back to move
  - `make_legal_action_mask(legal_moves)`: Create boolean mask for valid actions

##### **board_encoder.py**[8]
- **Class**: `CheckersBoardEncoder`
- **Purpose**: Converts board state to neural network input tensor
- **Encoding**: 5-channel (5, 8, 8) representation:
  1. **Channel 0**: Own regular pieces
  2. **Channel 1**: Own kings
  3. **Channel 2**: Opponent regular pieces
  4. **Channel 3**: Opponent kings
  5. **Channel 4**: Tempo indicator (whose turn)
- **Output**: `torch.Tensor` normalized to[1]

##### **buffer.py**[9]
- **Class**: `ReplayBuffer`
- **Purpose**: Experience replay storage optimized for RTX 2060
- **Capacity**: 100,000 transitions
- **Storage**: Pre-allocated NumPy arrays in CPU RAM (saves VRAM)
- **Stores**: (state, action, reward, next_state, done, next_legal_mask)
- **Features**:
  - Circular buffer with automatic overwriting
  - Legal action masks for Double DQN target computation
  - Efficient batch sampling to GPU
  - ~400 MB memory footprint

##### **move_parser.py**[10]
- **Functions**:
  - `parse_legal_moves()`: Normalize environment moves to action IDs
  - `is_capture_move()`: Detect capture vs simple move
  - `get_move_distance()`: Calculate Manhattan distance
  - `normalize_move_format()`: Convert lists to tuples

#### **D3QN Implementation** (`training/d3qn/`)

##### **model.py**[11]
- **Classes**: `DuelingDQN`, `D3QNModel`
- **Architecture**: Convolutional neural network with dueling streams

**Network Structure**:
```
Input: (batch, 5, 8, 8)
  ↓
Conv2d(5→32, 3×3) + ReLU
  ↓
Conv2d(32→64, 3×3) + ReLU
  ↓
Conv2d(64→64, 3×3) + ReLU
  ↓
Flatten → 4096 features
  ↓
LayerNorm(4096)
  ├─→ Value Stream: FC(4096→512) → FC(512→1)
  └─→ Advantage Stream: FC(4096→512) → FC(512→168)
  ↓
Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))
  ↓
Output: (batch, 168) × 0.1 scaling
```

**Key Features**:
- Dueling architecture separates state value from action advantages
- Kaiming initialization for numerical stability
- LayerNorm for feature stability
- 0.1× output scaling to prevent Q-value explosion
- ~3.5M trainable parameters

**D3QNModel Wrapper**:
- Maintains online and target networks
- Soft target updates with Polyak averaging (τ=0.005)

##### **trainer.py**[2]
- **Class**: `D3QNTrainer`
- **Purpose**: Complete training loop with Double DQN + Dueling architecture

**Training Algorithm**:
1. Sample batch from replay buffer
2. Compute current Q-values from online network
3. **Double DQN Target**:
   - Online network selects best legal action in next state
   - Target network evaluates that action
4. Compute TD target: `r + γ × Q_target(s', a*) × (1 - done)`
5. Backpropagate Huber loss
6. Gradient clipping (max_norm=0.1)
7. Soft update target network every step

**Key Methods**:
- `train_step(batch_size)`: Single optimization step
- `update_target_network()`: Polyak averaging update
- `collect_experience(num_steps, epsilon)`: Play games to fill buffer
- `save_checkpoint()` / `load_checkpoint()`: Training persistence

### 4. **Training Scripts**

#### **main.py**[3]
- **Primary Training Entry Point**
- **Hyperparameters**:
  - Episodes: 20,000
  - Batch size: 128
  - Learning rate: 2×10⁻⁵ (Adam)
  - Gamma: 0.99
  - Epsilon: 1.0 → 0.05 (linear decay over 2000 episodes)
  - Reward scaling: 1/100
  - Soft update τ: 0.005
- **Features**:
  - Resume training from checkpoints with automatic epsilon adjustment
  - Episode tracking and progress logging
  - Saves checkpoints every 500 episodes
  - "Gen 6.5 Logical Economy" reward system with empirical thresholds
- **Reward Processing**:
  - Win: +100.0 (scaled to +1.0)
  - Loss: -75.0 (scaled to -0.75)
  - Double jump/king capture (>20): +20.0
  - Single capture (>8): +5.0
  - Regular move: -0.2 (living tax)

#### **train_d3qn_checkers.py**[12]
- **Alternative training script** with different hyperparameters
- Step-based instead of episode-based (100K steps)
- Includes periodic evaluation against random agent
- Win rate tracking with best model saving

### 5. **Evaluation & Analysis Tools**

#### **evaluate.py**[13]
- **Purpose**: Visual game playback with trained agent
- **Usage**: `python evaluate.py --path checkpoint.pth --player 1 --speed 0.2`
- **Features**:
  - Play as either player (1=Red, 2=Black)
  - Real-time board rendering
  - Q-value display for agent moves
  - Win/loss/draw detection

#### **benchmark.py**[14]
- **Purpose**: Test all checkpoints against random agent
- **Process**:
  - Loads all `.pth` files from `checkpoints/`
  - Plays 25 games as Player 1 (Red)
  - Plays 25 games as Player 2 (Black)
  - Calculates total win percentage
- **Output**: Identifies champion model with highest win rate
- **Critical Fix**: Includes `find_matching_move()` to properly translate action IDs to environment moves (handles multi-jumps)

#### **diagnose_rewards.py**[15]
- **Purpose**: Empirical reward distribution analysis
- **Function**: Plays 100 random games and logs all rewards encountered
- **Output**: Min/max/unique reward values to tune reward scaling

#### **plot_logs.py**[16]
- **Purpose**: Visualize training progress from log files
- **Generates**: Dual plots (reward + loss) using matplotlib
- **Input**: `training_log.txt` with episode logs

## Technical Implementation Details

### Training Flow

1. **Initialization**: Environment, encoder, action manager, model, buffer, optimizer
2. **Episode Loop**:
   - Reset environment
   - **Agent Turn** (Player 1):
     - Encode board state → 5-channel tensor
     - Select action with ε-greedy policy
     - Map action ID → environment move format
     - Execute move, receive reward
     - Apply custom reward logic (Gen 6.5)
     - Store transition in buffer
     - Train if buffer size ≥ 1000
     - Soft update target network
   - **Opponent Turn** (Random agent):
     - Random legal move selection
     - No learning from opponent moves
3. **Logging**: Every 10 episodes with truthful outcome reporting
4. **Checkpointing**: Every 500 episodes with full state save

### Key Architectural Decisions

1. **Action Space Design**: 168 discrete actions cover all possible single-step moves (abstracts multi-jump complexity)[8]
2. **Legal Action Masking**: Prevents network from selecting illegal moves during training and inference[2]
3. **Double DQN**: Reduces Q-value overestimation by decoupling action selection and evaluation[2]
4. **Dueling Architecture**: Learns state value and action advantages separately for better generalization[11]
5. **Soft Updates**: Target network updates every step with τ=0.005 instead of hard copies[2]
6. **Reward Shaping**: Dense rewards with material tracking, positional incentives, and safety penalties[1]
7. **CPU Buffer Storage**: Keeps replay buffer in RAM, only transfers batches to GPU[9]

### Known Issues & Design Notes

- **MCTS Agent**: Not implemented (placeholder only)[4]
- **Multi-Jump Translation**: Requires `find_matching_move()` to correctly map network actions to environment capture sequences[14]
- **Player Encoding**: Agent always sees itself as positive pieces through perspective flipping[8]
- **Training Generations**: Comments reference "Gen 2-6" patches indicating iterative hyperparameter tuning[3]
- **Q-Value Monitoring**: Warning system for Q-value explosion (>100 or <-100)[2]

This is a production-ready RL system with comprehensive tooling for training, evaluation, debugging, and benchmarking checkers agents using modern deep reinforcement learning techniques.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/153436959/4acbeba9-b649-48c9-bf3b-6b8b5e02b296/env.py)
[2](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/153436959/08caaf24-2a83-461c-ad15-323713c3337d/trainer.py)
[3](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/153436959/02067f6a-232c-439c-95db-eac0bccc3ed2/main.py)
[4](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/153436959/83224919-321d-46e0-bd2d-0c5e6470052f/board.py)
[5](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/153436959/fd66a2d8-64c6-498a-b327-f2a663670949/rules.py)
[6](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/153436959/c997d3bb-00e6-4bf4-9819-2ef0424309ee/random_agent.py)
[7](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/153436959/5b949d99-bce0-40dd-8b4a-4b0f90e0686d/d3qn_agent.py)
[8](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/153436959/e02a5ede-528c-4a8c-a885-33b78f321722/init.py)
[9](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/153436959/bee37c53-fc28-45bf-b8c2-8aeff9d0e9c2/buffer.py)
[10](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/153436959/8c332b72-d1ad-4a7c-a019-141f7b3d62ad/move_parser.py)
[11](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/153436959/e3d096c1-a9a2-4bda-8486-bb5920916a11/model.py)
[12](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/153436959/6ef3e41e-55a4-4c4d-bcd3-879b13500804/train_d3qn_checkers.py)
[13](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/153436959/1a3c54c1-cc70-4275-ab3c-03b2729793e6/evaluate.py)
[14](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/153436959/ece6dd8b-73cf-4b6e-82ef-ff76208d10f0/benchmark.py)
[15](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/153436959/6f200d2e-56da-4e27-acee-be985625ee71/diagnose_rewards.py)
[16](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/153436959/77517f2b-5e7a-4944-bb51-905bcb6efd08/plot_logs.py)
[17](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/153436959/db3b8eb2-3b33-4d46-a1ef-0cf8034218b0/mcts_agent.py)
[18](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/153436959/0761b2a4-ce96-4335-9641-74dcf58d8c4d/init.py)
[19](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/153436959/f4385157-c677-4e46-b05d-06e1f0df98c4/init.py)
[20](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/153436959/342090c2-045e-440e-8d1c-74cacb865204/init.py)
