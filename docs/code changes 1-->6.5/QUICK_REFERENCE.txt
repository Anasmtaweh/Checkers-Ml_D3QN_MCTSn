
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                       D3QN CHECKERS - QUICK REFERENCE                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ FILE STRUCTURE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
training/
â”œâ”€â”€ common/
â”‚   â”œâ”€â”€ action_manager.py   - Universal action space (~168 actions)
â”‚   â”œâ”€â”€ board_encoder.py    - 5-channel CNN encoding
â”‚   â”œâ”€â”€ move_parser.py      - Move format utilities
â”‚   â””â”€â”€ buffer.py           - Pre-allocated replay buffer
â””â”€â”€ d3qn/
    â”œâ”€â”€ model.py            - Dueling DQN architecture
    â””â”€â”€ trainer.py          - Training loop & Double DQN

train_d3qn_checkers.py      - Ready-to-run training script

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸš€ TRAINING COMMANDS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# Start training with default settings
python train_d3qn_checkers.py

# Start with specific random seed
python train_d3qn_checkers.py --seed 42

# Resume from checkpoint (modify script to add --checkpoint arg)
# Add to script: checkpoint = args.checkpoint
# trainer.load_checkpoint(checkpoint)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš™ï¸ KEY HYPERPARAMETERS (in train_d3qn_checkers.py)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

class TrainingConfig:
    buffer_capacity = 100_000      # Replay buffer size
    batch_size = 64                # Training batch size
    learning_rate = 1e-4           # Adam learning rate
    gamma = 0.99                   # Discount factor

    epsilon_start = 1.0            # Initial exploration rate
    epsilon_end = 0.01             # Final exploration rate
    epsilon_decay = 0.9995         # Per-step decay multiplier

    target_update_freq = 1000      # Update target net every N steps
    total_steps = 100_000          # Total training steps

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ§© CODE SNIPPETS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

INITIALIZE COMPONENTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from training.common import ActionManager, CheckersBoardEncoder, ReplayBuffer
from training.d3qn import D3QNModel, D3QNTrainer
from checkers_env import CheckersEnv

env = CheckersEnv()
action_mgr = ActionManager(device="cuda")
encoder = CheckersBoardEncoder()
model = D3QNModel(action_dim=action_mgr.action_dim, device="cuda")
buffer = ReplayBuffer(capacity=100000, action_dim=action_mgr.action_dim)

INFERENCE (Use Trained Agent)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model.load("checkpoints/best_model.pt")
model.eval()

board = env.board.get_state()
player = env.current_player
legal_moves = env.get_legal_moves()

# Encode state
state = encoder.encode(board, player)

# Get Q-values
with torch.no_grad():
    q_values = model.get_q_values(state.unsqueeze(0))[0]

# Apply legal mask
mask = action_mgr.make_legal_action_mask(legal_moves)
masked_q = q_values.clone()
masked_q[~mask] = -1e9

# Select best action
action_id = masked_q.argmax().item()
move = action_mgr.get_move_from_id(action_id)

TRAINING STEP
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Collect experience
trainer.collect_experience(num_steps=4, epsilon=0.5)

# Train on batch
if len(buffer) >= 1000:
    loss = trainer.train_step(batch_size=64)

# Update target network
if step % 1000 == 0:
    trainer.update_target_network()

SAVE/LOAD
â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Save checkpoint
trainer.save_checkpoint("checkpoint.pt")

# Load checkpoint
trainer.load_checkpoint("checkpoint.pt")

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ” MONITORING TRAINING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

GOOD SIGNS:
  âœ“ Loss decreasing over time
  âœ“ Average reward increasing
  âœ“ Win rate vs random improving (target: 95%+)
  âœ“ Epsilon decaying smoothly

BAD SIGNS:
  âœ— Loss exploding (going to infinity)
  âœ— Reward stuck at negative values
  âœ— Win rate not improving after 20K steps
  âœ— NaN in Q-values or loss

TYPICAL LOG OUTPUT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Step    100 | Loss: 0.8234 | Reward:  -25.43 | Eps: 0.9500 | Buffer:    400
Step    200 | Loss: 0.7156 | Reward:  -18.21 | Eps: 0.9025 | Buffer:    800
Step   1000 | Loss: 0.4532 | Reward:   -5.67 | Eps: 0.6065 | Buffer:   4000
Step   5000 | Loss: 0.2341 | Reward:   12.34 | Eps: 0.2231 | Buffer:  20000
Step  10000 | Loss: 0.1823 | Reward:   25.67 | Eps: 0.0498 | Buffer:  40000

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ› TROUBLESHOOTING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

PROBLEM: Loss exploding
FIX:    learning_rate = 5e-5  (reduce)
        gradient_clip = 0.5   (increase)

PROBLEM: Out of GPU memory
FIX:    batch_size = 32       (reduce)
        Verify buffer uses CPU storage

PROBLEM: Training too slow
FIX:    Check CUDA is enabled
        Reduce log_freq
        Use fewer eval_episodes

PROBLEM: Not learning
FIX:    Check epsilon_decay (might be too slow)
        Increase min_buffer_size to 5000
        Verify legal masks are correct

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š PERFORMANCE EXPECTATIONS (RTX 2060)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Training speed: ~20-30 steps/second
100K steps:     ~1-2 hours
Memory usage:   
  - VRAM: ~1-2 GB
  - RAM:  ~500 MB (buffer + overhead)

After 10K steps:  Win rate ~40%, Reward ~0
After 50K steps:  Win rate ~90%, Reward ~25
After 100K steps: Win rate ~95%, Reward ~40

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ IMPORTANT CONCEPTS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Double DQN:
  Online network selects action, target network evaluates it.
  Reduces Q-value overestimation.

Dueling Architecture:
  Separates state value V(s) from action advantages A(s,a).
  Q(s,a) = V(s) + (A(s,a) - mean(A))

Legal Action Masking:
  Set Q-values of illegal actions to -1e9 before argmax.
  Critical for games with variable action spaces.

Canonicalization:
  Rotate board so network always sees itself as Player 1.
  Enables symmetric learning (2Ã— sample efficiency).

Target Network:
  Frozen copy of online network, updated every N steps.
  Provides stable training targets.

Epsilon-Greedy:
  With probability Îµ: random action (exploration)
  With probability 1-Îµ: best action (exploitation)
  Îµ decays over time: explore early, exploit later

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”— ARCHITECTURE SUMMARY
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Raw Board (8Ã—8)
    â†“ [BoardEncoder]
Encoded State (5, 8, 8)
    â†“ [CNN: 5â†’32â†’64â†’64]
Features (4096)
    â†“ [Dueling Streams]
Q-values (action_dim)
    â†“ [Legal Masking]
Best Action (int)
    â†“ [ActionManager]
Environment Move
    â†“ [env.step]
Next State + Reward

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“š DOCUMENTATION FILES:
  â€¢ PHASE_1_2_COMPLETE_SUMMARY.txt    - Phases 1 & 2 overview
  â€¢ D3QN_ARCHITECTURE.txt             - Model architecture details
  â€¢ PHASE_3_TRAINING_INFRASTRUCTURE.txt - Training system details
  â€¢ PHASE_3_COMPLETE_SUMMARY.txt      - Complete project summary

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Generated: December 18, 2025
