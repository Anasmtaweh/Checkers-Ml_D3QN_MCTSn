
================================================================================
PHASE 2: D3QN MODEL ARCHITECTURE - IMPLEMENTATION COMPLETE
================================================================================

Directory Structure:
--------------------
training/
├── __init__.py
├── common/
│   ├── __init__.py
│   ├── action_manager.py
│   ├── board_encoder.py
│   └── move_parser.py
└── d3qn/
    ├── __init__.py
    └── model.py              ← NEW: Neural network implementation

================================================================================
D3QN MODEL ARCHITECTURE
================================================================================

┌─────────────────────────────────────────────────────────────────────┐
│                         INPUT STATE                                  │
│                      Shape: (batch, 5, 8, 8)                        │
│                                                                      │
│  Channel 0: My Men      Channel 1: My Kings                         │
│  Channel 2: Enemy Men   Channel 3: Enemy Kings                      │
│  Channel 4: Tempo (0.0 = P1, 1.0 = P2)                             │
└─────────────────────────────────────────────────────────────────────┘
                                  ↓
┌─────────────────────────────────────────────────────────────────────┐
│                      CNN BACKBONE (Feature Extractor)                │
├─────────────────────────────────────────────────────────────────────┤
│  Conv2D: 5 → 32 channels, kernel=3x3, padding=1, ReLU              │
│  Output: (batch, 32, 8, 8)                                          │
├─────────────────────────────────────────────────────────────────────┤
│  Conv2D: 32 → 64 channels, kernel=3x3, padding=1, ReLU             │
│  Output: (batch, 64, 8, 8)                                          │
├─────────────────────────────────────────────────────────────────────┤
│  Conv2D: 64 → 64 channels, kernel=3x3, padding=1, ReLU             │
│  Output: (batch, 64, 8, 8)                                          │
├─────────────────────────────────────────────────────────────────────┤
│  Flatten: (batch, 64, 8, 8) → (batch, 4096)                        │
└─────────────────────────────────────────────────────────────────────┘
                                  ↓
                    ┌─────────────┴─────────────┐
                    ↓                           ↓
        ┌───────────────────────┐   ┌───────────────────────┐
        │   VALUE STREAM: V(s)  │   │  ADVANTAGE STREAM:    │
        │                       │   │      A(s, a)          │
        ├───────────────────────┤   ├───────────────────────┤
        │ Linear: 4096 → 512    │   │ Linear: 4096 → 512    │
        │ ReLU                  │   │ ReLU                  │
        ├───────────────────────┤   ├───────────────────────┤
        │ Linear: 512 → 1       │   │ Linear: 512 → action_ │
        │                       │   │                  dim   │
        │ Output: (batch, 1)    │   │ Output: (batch, ~168) │
        └───────────────────────┘   └───────────────────────┘
                    │                           │
                    └─────────────┬─────────────┘
                                  ↓
                    ┌─────────────────────────────────┐
                    │  DUELING AGGREGATION            │
                    │                                 │
                    │  Q(s,a) = V(s) +                │
                    │    (A(s,a) - mean(A(s,a)))      │
                    │                                 │
                    │  Output: (batch, action_dim)    │
                    └─────────────────────────────────┘

================================================================================
KEY DESIGN PRINCIPLES
================================================================================

1. DUELING ARCHITECTURE
   -------------------------
   Traditional DQN: Estimates Q(s, a) directly
   Dueling DQN:     Separates into V(s) and A(s, a)

   Benefits:
   ✓ Better state value estimation (useful when actions don't matter much)
   ✓ Faster learning (generalization across actions)
   ✓ More stable training (separate value and advantage gradients)

   Aggregation Formula:
   Q(s, a) = V(s) + (A(s, a) - mean_a[A(s, a)])

   The mean subtraction ensures identifiability: there's only one way
   to decompose Q into V and A, preventing the network from learning
   arbitrary offsets.

2. DOUBLE Q-LEARNING
   -------------------
   Problem: Standard DQN overestimates Q-values due to max operator

   Solution: Use two networks
   - Online network: Selects best action (argmax_a Q_online(s', a))
   - Target network: Evaluates that action (Q_target(s', argmax_a))

   This decorrelates action selection from evaluation, reducing bias.

3. CNN ARCHITECTURE
   ------------------
   Why CNNs for checkers?
   ✓ Translation invariance: Patterns (like piece formations) are
     similar regardless of board position
   ✓ Local feature extraction: Nearby squares matter most
   ✓ Hierarchical features: Low-level (piece presence) → High-level
     (tactical patterns)

   Design choices:
   - 3 conv layers: Sufficient for 8x8 board receptive field
   - 64 final channels: Rich feature representation
   - No pooling: Preserve spatial resolution (8x8 is already small)
   - Padding=1: Maintain board dimensions throughout

4. NETWORK SIZE
   -------------
   Total Parameters: ~1.2M trainable parameters

   Breakdown:
   - Conv layers: ~50K parameters
   - Value stream: ~2.1M parameters (4096 → 512 → 1)
   - Advantage stream: ~2.1M + action_dim parameters

   Size considerations:
   ✓ Large enough to learn complex patterns
   ✓ Small enough to train on CPU if needed
   ✓ Prevents overfitting on checkers domain

================================================================================
CLASS STRUCTURE
================================================================================

1. DuelingDQN (nn.Module)
   -----------------------
   Single network with dueling architecture.

   Methods:
   - __init__(action_dim, device): Initialize layers
   - forward(x): CNN → Dueling streams → Q-values
   - get_q_values(state): Forward pass with device handling

2. D3QNModel (Wrapper)
   --------------------
   Manages online and target networks for Double Q-learning.

   Attributes:
   - self.online: DuelingDQN for action selection & training
   - self.target: DuelingDQN for target Q-value computation

   Methods:
   - update_target_network(): Copy online → target weights
   - get_q_values(state, use_target): Get Q-values from either net
   - save(path): Save both networks
   - load(path): Load both networks

================================================================================
FORWARD PASS EXAMPLE
================================================================================

Input:
------
state = torch.FloatTensor([
    [...],  # Channel 0: My men
    [...],  # Channel 1: My kings  
    [...],  # Channel 2: Enemy men
    [...],  # Channel 3: Enemy kings
    [...]   # Channel 4: Tempo
])  # Shape: (5, 8, 8)

Forward Pass:
-------------
x = state.unsqueeze(0)              # (1, 5, 8, 8)

# CNN Backbone
x = ReLU(Conv1(x))                  # (1, 32, 8, 8)
x = ReLU(Conv2(x))                  # (1, 64, 8, 8)
x = ReLU(Conv3(x))                  # (1, 64, 8, 8)
x = Flatten(x)                      # (1, 4096)

# Dueling Streams
value = ReLU(Linear(x))             # (1, 512)
value = Linear(value)               # (1, 1)

advantage = ReLU(Linear(x))         # (1, 512)
advantage = Linear(advantage)       # (1, 168)

# Aggregation
adv_mean = advantage.mean(dim=1)    # (1,)
q_values = value + (advantage - adv_mean)  # (1, 168)

Output:
-------
q_values[0][42] = 0.753  # Q-value for action 42
q_values[0][89] = -0.231 # Q-value for action 89
...

================================================================================
INTEGRATION WITH EXISTING CODE
================================================================================

The d3qn_agent.py already references this model:

from training.d3qn.model import D3QNModel

Usage in Agent:
---------------
# Initialize
self.model = D3QNModel(
    action_dim=self.action_manager.action_dim,
    device=self.device
)

# Load weights
self.model.load(checkpoint_path)

# Get Q-values
state = self.encoder.encode(board, player, info)  # (5, 8, 8)
q_values = self.model.get_q_values(state)          # (1, action_dim)

# Apply legal move mask
masked_q = q_values.clone()
masked_q[~mask] = -1e9

# Select best action
action_id = torch.argmax(masked_q).item()

================================================================================
MATHEMATICAL FORMULATION
================================================================================

Standard DQN:
-------------
Q(s, a; θ) = Network(s, a; θ)

Loss: L(θ) = E[(r + γ max_a' Q(s', a'; θ^-) - Q(s, a; θ))²]

Dueling DQN:
------------
Q(s, a; θ) = V(s; θ_v) + A(s, a; θ_a) - (1/|A|) Σ_a' A(s, a'; θ_a)

Where:
- V(s; θ_v): Value stream parameters
- A(s, a; θ_a): Advantage stream parameters
- Mean subtraction enforces: E[A(s, a)] = 0

Double Q-Learning (Target Computation):
----------------------------------------
Standard DQN:       y = r + γ max_a' Q(s', a'; θ^-)
Double DQN:         y = r + γ Q(s', argmax_a' Q(s', a'; θ), θ^-)
                             ↑_target_net   ↑_online_net

This prevents overestimation by decoupling action selection from evaluation.

================================================================================
TRAINING CONSIDERATIONS (Phase 3 Preview)
================================================================================

Hyperparameters to tune:
------------------------
- Learning rate: 1e-4 to 1e-3
- Batch size: 32 to 128
- Target update frequency: Every 1000-10000 steps
- Replay buffer size: 50K to 500K transitions
- Epsilon decay: Start 1.0, decay to 0.01 over training

Loss function:
--------------
Huber loss (smooth L1) is recommended over MSE for stability:
L(δ) = {  0.5 * δ²        if |δ| ≤ 1
       {  |δ| - 0.5       otherwise

Gradient clipping:
------------------
Clip gradients to [-1, 1] or use gradient norm clipping to prevent
exploding gradients during training.

================================================================================
TESTING AND VALIDATION
================================================================================

Unit Tests Performed:
✓ Single state forward pass: (5, 8, 8) → (1, action_dim)
✓ Batch forward pass: (32, 5, 8, 8) → (32, action_dim)
✓ Target network initialization (matches online)
✓ Target network independence (doesn't update with online)
✓ Target network update (syncs with online on demand)
✓ Device handling (CPU/CUDA)
✓ Parameter counting (~1.2M parameters)

Integration Tests Needed:
□ End-to-end with BoardEncoder output
□ Gradient flow (ensure no vanishing/exploding gradients)
□ Q-value stability over training
□ Legal move masking with model output

================================================================================
NEXT STEPS (Phase 3)
================================================================================

With the model complete, implement:
1. training/d3qn/replay_buffer.py
   - Experience storage
   - Prioritized sampling (optional)
   - Efficient batch retrieval

2. training/d3qn/trainer.py
   - Training loop
   - Loss computation (Huber/MSE)
   - Optimizer (Adam)
   - Epsilon-greedy exploration
   - Target network updates
   - Logging and checkpointing

3. training/scripts/train_d3qn.py
   - Main training script
   - Hyperparameter configuration
   - Self-play or vs random agent

================================================================================
